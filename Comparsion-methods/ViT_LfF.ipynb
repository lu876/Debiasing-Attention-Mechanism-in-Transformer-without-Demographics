{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11022dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenyu/miniconda3/envs/DLcourse/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as tvt\n",
    "from torchvision.datasets import CelebA\n",
    "from typing import Any, Callable, Optional, Tuple, Union, List\n",
    "#step 1 import image\n",
    "%matplotlib inline\n",
    "import torchvision.datasets\n",
    "import math\n",
    "import torchvision.transforms as tvt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTConfig, ViTModel\n",
    "\n",
    "image_size = 64\n",
    "batch_size = 256\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "class CelebAWithIndex(CelebA):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            split: str = \"train\",\n",
    "            target_type: Union[List[str], str] = \"attr\",\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            download: bool = False,\n",
    "    ) -> None:\n",
    "        super(CelebAWithIndex, self).__init__(\n",
    "            root,\n",
    "            split=split,\n",
    "            target_type=target_type,\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "            download=download\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any, Any]:\n",
    "        X, y = super(CelebAWithIndex, self).__getitem__(index)\n",
    "\n",
    "        return index, X, y\n",
    "\n",
    "dataset = CelebAWithIndex(\"../../celeba/datasets/\",split='train', transform=tvt.Compose([\n",
    "                              tvt.Resize((image_size,image_size)),\n",
    "                              tvt.ToTensor(),\n",
    "                              tvt.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                            std=[0.5, 0.5, 0.5])\n",
    "                          ]))\n",
    "\n",
    "test_dataset = CelebAWithIndex(\"../../celeba/datasets/\",split='test', transform=tvt.Compose([\n",
    "                              tvt.Resize((image_size,image_size)),\n",
    "                              tvt.ToTensor(),\n",
    "                              tvt.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                            std=[0.5, 0.5, 0.5])\n",
    "                          ]))\n",
    "\n",
    "training_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3abed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, vit):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.vit = vit\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.vit(x)\n",
    "        m = z.last_hidden_state\n",
    "        g = m[:,0]\n",
    "        y = self.seq(g)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#test code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "class MultiDimAverageMeter(object):\n",
    "    def __init__(self, dims):\n",
    "        self.dims = dims\n",
    "        self.cum = torch.zeros(np.prod(dims))\n",
    "        self.cnt = torch.zeros(np.prod(dims))\n",
    "        self.idx_helper = torch.arange(np.prod(dims), dtype=torch.long).reshape(\n",
    "            *dims\n",
    "        )\n",
    "\n",
    "    def add(self, vals, idxs):\n",
    "        flattened_idx = torch.stack(\n",
    "            [self.idx_helper[tuple(idxs[i])] for i in range(idxs.size(0))],\n",
    "            dim=0,\n",
    "        )\n",
    "        self.cum.index_add_(0, flattened_idx, vals.view(-1).float())\n",
    "        self.cnt.index_add_(\n",
    "            0, flattened_idx, torch.ones_like(vals.view(-1), dtype=torch.float)\n",
    "        )\n",
    "        \n",
    "    def get_mean(self):\n",
    "        return (self.cum / self.cnt).reshape(*self.dims)\n",
    "\n",
    "    def reset(self):\n",
    "        self.cum.zero_()\n",
    "        self.cnt.zero_()\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    \n",
    "    def __init__(self, label, alpha=0.9):\n",
    "        self.label = label\n",
    "        self.alpha = alpha\n",
    "        self.parameter = torch.zeros(label.size(0))\n",
    "        self.updated = torch.zeros(label.size(0))\n",
    "        \n",
    "    def update(self, data, index):\n",
    "        self.parameter[index] = self.alpha * self.parameter[index] + (1-self.alpha*self.updated[index]) * data\n",
    "        self.updated[index] = 1\n",
    "        \n",
    "    def max_loss(self, label):\n",
    "        label_index = np.where(self.label == label)[0]\n",
    "        return self.parameter[label_index].max()\n",
    "\n",
    "class GeneralizedCELoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, q=0.3):\n",
    "        super(GeneralizedCELoss, self).__init__()\n",
    "        self.q = q\n",
    "             \n",
    "    def forward(self, logits, targets):\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        if np.isnan(p.mean().item()):\n",
    "            raise NameError('GCE_p')\n",
    "        Yg = torch.gather(p, 1, torch.unsqueeze(targets, 1))\n",
    "        # modify gradient of cross entropy\n",
    "        loss_weight = (Yg.squeeze().detach()**self.q)*self.q\n",
    "        if np.isnan(Yg.mean().item()):\n",
    "            raise NameError('GCE_Yg')\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets, reduction='none') * loss_weight\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    log_dir,\n",
    "    target_attr_idx,\n",
    "    bias_attr_idx,\n",
    "    main_num_steps,\n",
    "    main_valid_freq,\n",
    "    main_optimizer_tag,\n",
    "    main_learning_rate,\n",
    "    main_weight_decay,\n",
    "):\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    writer = SummaryWriter(os.path.join(log_dir, \"summary\", 'CelebA'))\n",
    "    \n",
    "    # define model and optimizer\n",
    "    configuration = ViTConfig(num_hidden_layers = 8, num_attention_heads = 8, \n",
    "                          intermediate_size = 768, image_size= 64, patch_size = 16)\n",
    "    vit = ViTModel(configuration)\n",
    "    configuration = vit.config\n",
    "    vit = vit.to(device)\n",
    "    model_1 = VisionTransformer(vit)\n",
    "    model_2 = VisionTransformer(vit)\n",
    "    model_b = model_1.to(device)\n",
    "    model_d = model_2.to(device)\n",
    "    \n",
    "    if main_optimizer_tag == \"Adam\":\n",
    "        optimizer_b = torch.optim.Adam(\n",
    "            model_b.parameters(),\n",
    "            lr=main_learning_rate,\n",
    "            weight_decay=main_weight_decay,\n",
    "        )\n",
    "        optimizer_d = torch.optim.Adam(\n",
    "            model_d.parameters(),\n",
    "            lr=main_learning_rate,\n",
    "            weight_decay=main_weight_decay,\n",
    "        )\n",
    "    elif main_optimizer_tag == \"AdamW\":\n",
    "        optimizer_b = torch.optim.AdamW(\n",
    "            model_b.parameters(),\n",
    "            lr=main_learning_rate,\n",
    "            weight_decay=main_weight_decay,\n",
    "        )\n",
    "        optimizer_d = torch.optim.AdamW(\n",
    "            model_d.parameters(),\n",
    "            lr=main_learning_rate,\n",
    "            weight_decay=main_weight_decay,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # define loss\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    bias_criterion = GeneralizedCELoss()\n",
    "    \n",
    "    train_target_attr = dataset.attr[:, target_attr_idx]\n",
    "    train_bias_attr = dataset.attr[:, bias_attr_idx]\n",
    "    attr_dims = []\n",
    "    attr_dims.append(torch.max(train_target_attr).item() + 1)\n",
    "    attr_dims.append(torch.max(train_bias_attr).item() + 1)\n",
    "    num_classes = attr_dims[0]\n",
    "    print('num',num_classes)\n",
    "    \n",
    "\n",
    "    sample_loss_ema_b = EMA(torch.LongTensor(train_target_attr), alpha=0.7)\n",
    "    sample_loss_ema_d = EMA(torch.LongTensor(train_target_attr), alpha=0.7)\n",
    "\n",
    "    # define evaluation function\n",
    "    def evaluate(model, data_loader):\n",
    "        print('valid')\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        \n",
    "        attrwise_acc_meter = MultiDimAverageMeter(attr_dims)\n",
    "        for index, data, attr in tqdm(data_loader, leave=False):\n",
    "            label = attr[:, target_attr_idx]\n",
    "            data = data.to(device)\n",
    "            attr = attr.to(device)\n",
    "            label = label.to(device)\n",
    "            test_gt.extend(label.detach().cpu().numpy())\n",
    "            with torch.no_grad():\n",
    "                logit = model(data)\n",
    "                pred = logit.data.max(1, keepdim=True)[1].squeeze(1)\n",
    "                correct = (pred == label).long()\n",
    "                \n",
    "                test_pred_ = torch.argmax(logit, dim=1)\n",
    "                test_pred.extend(test_pred_.detach().cpu().numpy())\n",
    "                \n",
    "\n",
    "            attr = attr[:, [target_attr_idx, bias_attr_idx]]\n",
    "\n",
    "            attrwise_acc_meter.add(correct.cpu(), attr.cpu())\n",
    "\n",
    "        accs = attrwise_acc_meter.get_mean()\n",
    "        ACC = accuracy_score(test_gt, test_pred)\n",
    "        print('Test ACC', ACC)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        return accs\n",
    "\n",
    "    # jointly training biased/de-biased model\n",
    "    valid_attrwise_accs_list = []\n",
    "    num_updated = 0\n",
    "    \n",
    "    for step in tqdm(range(main_num_steps)):\n",
    "        # train main model\n",
    "        try:\n",
    "            index, data, attr = next(train_iter)\n",
    "        except:\n",
    "            train_iter = iter(training_data_loader)\n",
    "            index, data, attr = next(train_iter)\n",
    "\n",
    "        data = data.to(device)\n",
    "        attr = attr.to(device)\n",
    "        label = attr[:, target_attr_idx]\n",
    "        bias_label = attr[:, bias_attr_idx]\n",
    "        \n",
    "        logit_b = model_b(data)\n",
    "        if np.isnan(logit_b.mean().item()):\n",
    "            print(logit_b)\n",
    "            raise NameError('logit_b')\n",
    "        logit_d = model_d(data)\n",
    "        \n",
    "        loss_b = criterion(logit_b, label).cpu().detach()\n",
    "        loss_d = criterion(logit_d, label).cpu().detach()\n",
    "                \n",
    "        if np.isnan(loss_b.mean().item()):\n",
    "            raise NameError('loss_b')\n",
    "        if np.isnan(loss_d.mean().item()):\n",
    "            raise NameError('loss_d')\n",
    "        \n",
    "        loss_per_sample_b = loss_b\n",
    "        loss_per_sample_d = loss_d\n",
    "        \n",
    "        # EMA sample loss\n",
    "        sample_loss_ema_b.update(loss_b, index)\n",
    "        sample_loss_ema_d.update(loss_d, index)\n",
    "        \n",
    "        # class-wise normalize\n",
    "        loss_b = sample_loss_ema_b.parameter[index].clone().detach()\n",
    "        loss_d = sample_loss_ema_d.parameter[index].clone().detach()\n",
    "        \n",
    "        if np.isnan(loss_b.mean().item()):\n",
    "            raise NameError('loss_b_ema')\n",
    "        if np.isnan(loss_d.mean().item()):\n",
    "            raise NameError('loss_d_ema')\n",
    "        \n",
    "        label_cpu = label.cpu()\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            class_index = np.where(label_cpu == c)[0]\n",
    "            max_loss_b = sample_loss_ema_b.max_loss(c)\n",
    "            max_loss_d = sample_loss_ema_d.max_loss(c)\n",
    "            loss_b[class_index] /= max_loss_b\n",
    "            loss_d[class_index] /= max_loss_d\n",
    "            \n",
    "        # re-weighting based on loss value / generalized CE for biased model\n",
    "        loss_weight = loss_b / (loss_b + loss_d + 1e-8)\n",
    "        if np.isnan(loss_weight.mean().item()):\n",
    "            raise NameError('loss_weight')\n",
    "            \n",
    "        loss_b_update = bias_criterion(logit_b, label)\n",
    "\n",
    "        if np.isnan(loss_b_update.mean().item()):\n",
    "            raise NameError('loss_b_update')\n",
    "        loss_d_update = criterion(logit_d, label) * loss_weight.to(device)\n",
    "        if np.isnan(loss_d_update.mean().item()):\n",
    "            raise NameError('loss_d_update')\n",
    "        loss = loss_b_update.mean() + loss_d_update.mean()\n",
    "        \n",
    "        num_updated += loss_weight.mean().item() * data.size(0)\n",
    "\n",
    "        optimizer_b.zero_grad()\n",
    "        optimizer_d.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_b.step()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        main_log_freq = 100\n",
    "        if step % main_log_freq == 0:\n",
    "        \n",
    "            writer.add_scalar(\"loss/b_train\", loss_per_sample_b.mean(), step)\n",
    "            writer.add_scalar(\"loss/d_train\", loss_per_sample_d.mean(), step)\n",
    "\n",
    "            bias_attr = attr[:, bias_attr_idx]\n",
    "\n",
    "            aligned_mask = (label == bias_attr)\n",
    "            skewed_mask = (label != bias_attr)\n",
    "            \n",
    "            writer.add_scalar('loss_variance/b_ema', sample_loss_ema_b.parameter.var(), step)\n",
    "            writer.add_scalar('loss_std/b_ema', sample_loss_ema_b.parameter.std(), step)\n",
    "            writer.add_scalar('loss_variance/d_ema', sample_loss_ema_d.parameter.var(), step)\n",
    "            writer.add_scalar('loss_std/d_ema', sample_loss_ema_d.parameter.std(), step)\n",
    "\n",
    "            if aligned_mask.any().item():\n",
    "                writer.add_scalar(\"loss/b_train_aligned\", loss_per_sample_b[aligned_mask].mean(), step)\n",
    "                writer.add_scalar(\"loss/d_train_aligned\", loss_per_sample_d[aligned_mask].mean(), step)\n",
    "                writer.add_scalar('loss_weight/aligned', loss_weight[aligned_mask].mean(), step)\n",
    "\n",
    "            if skewed_mask.any().item():\n",
    "                writer.add_scalar(\"loss/b_train_skewed\", loss_per_sample_b[skewed_mask].mean(), step)\n",
    "                writer.add_scalar(\"loss/d_train_skewed\", loss_per_sample_d[skewed_mask].mean(), step)\n",
    "                writer.add_scalar('loss_weight/skewed', loss_weight[skewed_mask].mean(), step)\n",
    "\n",
    "        if step % main_valid_freq == 0:\n",
    "            valid_attrwise_accs_b = evaluate(model_b, valid_loader)\n",
    "            valid_attrwise_accs_d = evaluate(model_d, valid_loader)\n",
    "            valid_attrwise_accs_list.append(valid_attrwise_accs_d)\n",
    "            valid_accs_b = torch.mean(valid_attrwise_accs_b)\n",
    "            writer.add_scalar(\"acc/b_valid\", valid_accs_b, step)\n",
    "            valid_accs_d = torch.mean(valid_attrwise_accs_d)\n",
    "            writer.add_scalar(\"acc/d_valid\", valid_accs_d, step)\n",
    "            print('valid_accs_d',valid_accs_d)\n",
    "\n",
    "            eye_tsr = torch.eye(attr_dims[0]).long()\n",
    "            \n",
    "            writer.add_scalar(\n",
    "                \"acc/b_valid_aligned\",\n",
    "                valid_attrwise_accs_b[eye_tsr == 1].mean(),\n",
    "                step,\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"acc/b_valid_skewed\",\n",
    "                valid_attrwise_accs_b[eye_tsr == 0].mean(),\n",
    "                step,\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"acc/d_valid_aligned\",\n",
    "                valid_attrwise_accs_d[eye_tsr == 1].mean(),\n",
    "                step,\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"acc/d_valid_skewed\",\n",
    "                valid_attrwise_accs_d[eye_tsr == 0].mean(),\n",
    "                step,\n",
    "            )\n",
    "            \n",
    "            num_updated_avg = num_updated / batch_size / main_valid_freq\n",
    "            writer.add_scalar(\"num_updated/all\", num_updated_avg, step)\n",
    "            num_updated = 0\n",
    "\n",
    "    os.makedirs(os.path.join(log_dir, \"result\"), exist_ok=True)\n",
    "    result_path = os.path.join(log_dir, \"result\", \"result.th\")\n",
    "    model_path = os.path.join(log_dir, \"result\", \"model.th\")\n",
    "    valid_attrwise_accs_list = torch.stack(valid_attrwise_accs_list)\n",
    "    print('valid_attrwise_accs_list',valid_attrwise_accs_list)\n",
    "    with open(result_path, \"wb\") as f:\n",
    "        torch.save({\"valid/attrwise_accs\": valid_attrwise_accs_list}, f)\n",
    "    state_dict = {\n",
    "        #'steps': step, \n",
    "        'state_dict': model_d.state_dict(), \n",
    "        'state_dict_b': model_b.state_dict(),\n",
    "        #'optimizer': optimizer_d.state_dict(), \n",
    "    }\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        torch.save(state_dict, f)\n",
    "    \n",
    "it = int(len(dataset)/batch_size)\n",
    "train(log_dir=r'./log', target_attr_idx=2, bias_attr_idx=20, main_num_steps=636 * 200, main_valid_freq=636, main_optimizer_tag=\"Adam\",main_learning_rate=5e-5,\n",
    "    main_weight_decay=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4996d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(len(dataset)/batch_size)*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd70f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8203907815631263\n",
      "male TPR 0.6718913270637409\n",
      "DP 0.3459720451596834\n",
      "EOP 0.1484994544993854\n",
      "EoD 0.1979034947919354\n",
      "acc 0.7087466185752931\n",
      "Trade off 0.5684831858372758\n"
     ]
    }
   ],
   "source": [
    "model_path = './log/result/model.th'\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "\n",
    "configuration = ViTConfig(num_hidden_layers = 8, num_attention_heads = 8, \n",
    "                          intermediate_size = 768, image_size= 64, patch_size = 16)\n",
    "vit = ViTModel(configuration)\n",
    "configuration = vit.config\n",
    "vit = vit.to(device)\n",
    "\n",
    "model_d = VisionTransformer(vit)\n",
    "model_d.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model_d = model_d.to(device)\n",
    "\n",
    "test_pred = []\n",
    "test_gt = []\n",
    "sense_gt = []\n",
    "female_predic = []\n",
    "female_gt = []\n",
    "male_predic = []\n",
    "male_gt = []\n",
    "\n",
    "\n",
    "model_d.eval()\n",
    "# Evaluate on test set.\n",
    "for step, (index, test_input, attributes) in enumerate(valid_loader):\n",
    "    sensitive, test_target = attributes[:,20], attributes[:,2]\n",
    "    test_input = test_input.to(device)\n",
    "    test_target = test_target.to(device)\n",
    "\n",
    "    gt = test_target.detach().cpu().numpy()\n",
    "    sen = sensitive.detach().cpu().numpy()\n",
    "    test_gt.extend(gt)\n",
    "    sense_gt.extend(sen)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction= model_d(test_input)\n",
    "        test_pred_ = torch.argmax(prediction, dim=1)\n",
    "        test_pred.extend(test_pred_.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "for i in range(len(sense_gt)):\n",
    "    if sense_gt[i] == 0:\n",
    "        female_predic.append(test_pred[i])\n",
    "        female_gt.append(test_gt[i])\n",
    "    else:\n",
    "        male_predic.append(test_pred[i])\n",
    "        male_gt.append(test_gt[i])\n",
    "female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "male_CM = confusion_matrix(male_gt, male_predic) \n",
    "female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0])\n",
    "female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "\n",
    "print('Female TPR', female_TPR)\n",
    "print('male TPR', male_TPR)\n",
    "print('DP',abs(female_dp - male_dp))\n",
    "print('EOP', abs(female_TPR - male_TPR))\n",
    "print('EoD',0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR)))\n",
    "print('acc', accuracy_score(test_gt, test_pred))\n",
    "print('Trade off',accuracy_score(test_gt, test_pred)*(1-0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR))) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
