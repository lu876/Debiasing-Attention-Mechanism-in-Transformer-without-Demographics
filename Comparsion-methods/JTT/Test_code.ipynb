{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f479f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenyu/miniconda3/envs/DLcourse/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#step 1 import image\n",
    "%matplotlib inline\n",
    "import torchvision.datasets\n",
    "import math\n",
    "import torchvision.transforms as tvt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 64\n",
    "batch_size = 128\n",
    "\n",
    "test_dataset = torchvision.datasets.CelebA(\"../../../celeba/datasets/\",split='test', transform=tvt.Compose([\n",
    "                                #tvt.CenterCrop(image_size),\n",
    "                                tvt.Resize((image_size,image_size)),\n",
    "                                tvt.ToTensor(),\n",
    "                                tvt.Normalize(mean=(0.5000, 0.5000, 0.5000), std=(0.5000, 0.5000, 0.5000)),\n",
    "                                ]))\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b5fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "configuration = ViTConfig(num_hidden_layers = 8, num_attention_heads = 8, \n",
    "                          intermediate_size = 768, image_size= image_size, patch_size = 16)\n",
    "vit = ViTModel(configuration)\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, vit):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.vit = vit\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.vit(x)\n",
    "        m = z.last_hidden_state\n",
    "        g = m[:,0]\n",
    "        y = self.fc(g)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdadc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 156/156 [00:16<00:00,  9.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9118  649]\n",
      " [ 522 1958]]\n",
      "[[7362  173]\n",
      " [ 103   77]]\n",
      "Female TPR 0.7895161290322581\n",
      "male TPR 0.42777777777777776\n",
      "DP 0.1804640505820966\n",
      "EOP 0.36173835125448034\n",
      "EoD 0.20261353655605882\n",
      "acc 0.9275122733193066\n",
      "Trade off 0.739585731422932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "        \n",
    "def test(test_loader, model, print_fairness):\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "        with torch.no_grad():\n",
    "            with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "                for content in tepoch:\n",
    "                    test_images, test_attributes = content\n",
    "                    sensitive, label = test_attributes[:,20], test_attributes[:,9]\n",
    "                    prediction = model(test_images.to(device))\n",
    "                    label = label.to(torch.float).to(device)\n",
    "                    prediction = torch.argmax(prediction, dim=1)\n",
    "                    gt = label.detach().cpu().numpy()\n",
    "                    sen = sensitive.detach().cpu().numpy()\n",
    "                    test_pred.extend(prediction.squeeze().detach().cpu().numpy())\n",
    "                    test_gt.extend(gt)\n",
    "                    sense_gt.extend(sen)\n",
    "            for i in range(len(sense_gt)):\n",
    "                if sense_gt[i] == 0:\n",
    "                    female_predic.append(test_pred[i])\n",
    "                    female_gt.append(test_gt[i])\n",
    "                else:\n",
    "                    male_predic.append(test_pred[i])\n",
    "                    male_gt.append(test_gt[i])\n",
    "                    \n",
    "            female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "            male_CM = confusion_matrix(male_gt, male_predic) \n",
    "            female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "            male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "            female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "            male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0])\n",
    "            female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "            male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "            if print_fairness == True:\n",
    "                print(female_CM)\n",
    "                print(male_CM)\n",
    "                print('Female TPR', female_TPR)\n",
    "                print('male TPR', male_TPR)\n",
    "                print('DP',abs(female_dp - male_dp))\n",
    "                print('EOP', abs(female_TPR - male_TPR))\n",
    "                print('EoD',0.5*(abs(female_FPR-male_FPR)+abs(female_TPR-male_TPR)))\n",
    "                print('acc', accuracy_score(test_gt, test_pred))\n",
    "                print('Trade off',accuracy_score(test_gt, test_pred)*(1-0.5*(abs(female_FPR-male_FPR)+abs(female_TPR-male_TPR))) )\n",
    " \n",
    "        \n",
    "def main():   \n",
    "    model = VisionTransformer(vit)\n",
    "    pre_trained_weight='results/CelebA/CelebA_sample_exp/train_downstream_ERM_upweight_0_epochs_50_lr_0.0001_weight_decay_0.0001/final_epoch1/JTT_upweight_50_epochs_50_lr_0.0001_weight_decay_0.0001/model_outputs/20_model.pth'\n",
    "    #pre_trained_weight='results/CelebA/CelebA_sample_exp/ERM_upweight_0_epochs_50_lr_0.0001_weight_decay_0.1/model_outputs/30_model.pth'\n",
    "    model = torch.load(pre_trained_weight, map_location=device)\n",
    "    model = model.to(device)\n",
    "    test(test_data_loader, model, print_fairness=True)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
