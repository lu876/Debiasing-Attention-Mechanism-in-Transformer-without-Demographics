{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a8b1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138354 24416\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#step 1 import image\n",
    "%matplotlib inline\n",
    "import torchvision.datasets\n",
    "import math\n",
    "import torchvision.transforms as tvt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTConfig, ViTModel\n",
    "from super_con import SupConLoss\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "device_id =1\n",
    "\"\"\"\n",
    "data_root = \"../celeba/datasets\"\n",
    "\n",
    "base_url = \"https://graal.ift.ulaval.ca/public/celeba/\"\n",
    "\n",
    "file_list = [\n",
    "    \"img_align_celeba.zip\",\n",
    "    \"list_attr_celeba.txt\",\n",
    "    \"identity_CelebA.txt\",\n",
    "    \"list_bbox_celeba.txt\",\n",
    "    \"list_landmarks_align_celeba.txt\",\n",
    "    \"list_eval_partition.txt\",\n",
    "]\n",
    "\n",
    "# Path to folder with the dataset\n",
    "dataset_folder = f\"{data_root}/celeba\"\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "for file in file_list:\n",
    "    url = f\"{base_url}/{file}\"\n",
    "    if not os.path.exists(f\"{dataset_folder}/{file}\"):\n",
    "        wget.download(url, f\"{dataset_folder}/{file}\")\n",
    "\n",
    "with zipfile.ZipFile(f\"{dataset_folder}/img_align_celeba.zip\", \"r\") as ziphandler:\n",
    "    ziphandler.extractall(dataset_folder)\n",
    "\"\"\"\n",
    "\n",
    "image_size = 64\n",
    "batch_size = 256\n",
    "dataset = torchvision.datasets.CelebA(\"../celeba/datasets/\",split='train', transform=tvt.Compose([\n",
    "                                  tvt.Resize((image_size,image_size)),\n",
    "                                  tvt.ToTensor(),\n",
    "                                  tvt.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                std=[0.5, 0.5, 0.5])\n",
    "                              ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.CelebA(\"../celeba/datasets/\",split='test', transform=tvt.Compose([\n",
    "                                  tvt.Resize((image_size,image_size)),\n",
    "                                  tvt.ToTensor(),\n",
    "                                  tvt.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                std=[0.5, 0.5, 0.5])\n",
    "                              ]))\n",
    "\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.85 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "training_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002fcdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 17, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTConfig, ViTModel\n",
    "configuration = ViTConfig(num_hidden_layers = 8, num_attention_heads = 8, \n",
    "                          intermediate_size = 768, image_size= 64, patch_size = 16)\n",
    "model = ViTModel(configuration)\n",
    "configuration = model.config\n",
    "t = iter(test_data_loader)\n",
    "img, label = next(t)\n",
    "img\n",
    "y = model(img)\n",
    "y.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217b9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_value_patch(attention, value, k):\n",
    "    \"\"\"\n",
    "    Get top-k attention values based on average attention weights across heads.\n",
    "\n",
    "    Parameters:\n",
    "    - attention (tensor): Shape [Batch, Head, token]\n",
    "    - value (tensor): Shape [Batch, token, dim]\n",
    "    - k (int): Number of top attention values to select\n",
    "\n",
    "    Returns:\n",
    "    - top_k_values (tensor): Shape [Batch, k, dim]\n",
    "    \"\"\"\n",
    "\n",
    "    # Average attention across the head dimension.\n",
    "    avg_attention = attention.mean(dim=1)\n",
    "\n",
    "    # Get the top-k attention indices.\n",
    "    _, top_k_indices = avg_attention.topk(k, dim=-1)\n",
    "\n",
    "    # Gather the top-k values.\n",
    "    batch_size, _ = top_k_indices.shape\n",
    "    batch_indices = torch.arange(batch_size)[:, None].to(top_k_indices.device)\n",
    "    top_k_values = value[batch_indices, top_k_indices].view(batch_size, k, -1)\n",
    "\n",
    "    return top_k_values\n",
    "\n",
    "    \n",
    "class Last_Attention(nn.Module):\n",
    "    def __init__(self, normalize):\n",
    "        super(Last_Attention, self).__init__()\n",
    "        self.normalize = normalize\n",
    "        self.p_dim = 2\n",
    "        self.emb_size = 768\n",
    "        self.head = 8\n",
    "        self.temperature = 1\n",
    "        self.head_dim = self.emb_size //self.head\n",
    "        self.Q = nn.Linear(768,768)\n",
    "        self.K = nn.Linear(768,768)\n",
    "        self.V = nn.Linear(768,768)\n",
    "        self.projection = nn.Linear(768, 768)\n",
    "        \n",
    "        self.soft_max = nn.Softmax(dim=-1)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.p_dim*768, 512, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128, bias=False),\n",
    "        )\n",
    "        self.momentum = 0.1\n",
    "        self.register_buffer('running_mean_q', torch.zeros(1,8,17,96))\n",
    "        self.register_buffer('running_std_q', torch.ones(1,8,17,96))\n",
    "        self.register_buffer('running_mean_k', torch.zeros(1,8,17,96))\n",
    "        self.register_buffer('running_std_k', torch.ones(1,8,17,96))\n",
    "\n",
    "    def register_buffer(self, name, tensor):\n",
    "        setattr(self, name, tensor)\n",
    "    \n",
    "        \n",
    "    def forward(self, x, training):\n",
    "        B, N, C = x.shape\n",
    "        origin_k = self.K(x)\n",
    "        origin_q = self.Q(x)\n",
    "        origin_v = self.V(x)\n",
    "        \n",
    "        q = origin_q.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        k = origin_k.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        v = origin_v.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        \n",
    "        if self.normalize == True:\n",
    "            self.running_mean_q = self.running_mean_q.detach()\n",
    "            self.running_std_q = self.running_std_q.detach()\n",
    "            self.running_mean_k = self.running_mean_k.detach()\n",
    "            self.running_std_k = self.running_std_k.detach()\n",
    "            \n",
    "            if training:\n",
    "                q_mean, q_std = torch.mean(q, 0, keepdim=True), torch.std(q, 0, keepdim=True)\n",
    "                k_mean, k_std = torch.mean(k, 0, keepdim=True), torch.std(k, 0, keepdim=True)  \n",
    "\n",
    "                self.running_mean_q = (1 - self.momentum) * self.running_mean_q.to(device) + self.momentum * q_mean\n",
    "                self.running_std_q = (1 - self.momentum) * self.running_std_q.to(device) + self.momentum * q_std\n",
    "                self.running_mean_k = (1 - self.momentum) * self.running_mean_k.to(device) + self.momentum * k_mean\n",
    "                self.running_std_k = (1 - self.momentum) * self.running_std_k.to(device) + self.momentum * k_std\n",
    "                    \n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_mean = self.running_mean_q.to(device)\n",
    "                    q_std = self.running_std_q.to(device)\n",
    "                    k_mean = self.running_mean_k.to(device)\n",
    "                    k_std = self.running_std_k.to(device)\n",
    "        \n",
    "        q = (q - q_mean)  / q_std\n",
    "        q = torch.abs(q)\n",
    "        k = (k - k_mean) / k_std\n",
    "        k = torch.abs(k)\n",
    "        \n",
    "        attention = (q @ k.transpose(-2,-1))* (self.head_dim ** (-0.5))\n",
    "        atten = self.soft_max(attention/self.temperature)\n",
    "        out = (atten @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.projection(out)\n",
    "        attentions = atten[:,:, 0, :]\n",
    "        v = v.transpose(1, 2).reshape(B, N, C)\n",
    "        mst_val = choose_value_patch(attentions, v, self.p_dim)\n",
    "        mst_val = mst_val.reshape(B, -1)\n",
    "        mst_val = self.projector(mst_val)\n",
    "        z = F.normalize(mst_val, dim=1)\n",
    "        return out, z, atten\n",
    "\n",
    "\n",
    "    \n",
    "class Last_ATBlock(nn.Module):\n",
    "    def __init__(self, normalize):\n",
    "        super().__init__()\n",
    "        dim = 768\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = Last_Attention(normalize)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(768, 768)          \n",
    "        )\n",
    "        \n",
    "    def forward(self, x, training):\n",
    "        identity = x\n",
    "        x = self.norm(x)\n",
    "        x, vz, att = self.attention(x, training)\n",
    "        x += identity\n",
    "        res = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x += res\n",
    "        return x, vz, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20b4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, vit, normalize):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.vit = vit\n",
    "        self.last_encoder = Last_ATBlock(normalize)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 1),     \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, training):\n",
    "        z = self.vit(x)\n",
    "        m = z.last_hidden_state\n",
    "        m, vz, att = self.last_encoder(m, training)\n",
    "        g = m[:,0]\n",
    "        y = self.seq(g)\n",
    "        return y, vz , att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fdc3e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  \n",
    "\n",
    "\n",
    "def test_epoch(model, p=False):\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    test_gt = []\n",
    "    sense_gt = []\n",
    "    a0_predic = []\n",
    "    a0_gt = []\n",
    "    a1_predic = []\n",
    "    a1_gt = []\n",
    "    criterion = nn.BCELoss()\n",
    "    testing_loss = 0.0\n",
    "    for step, (test_input, attributes) in enumerate(test_data_loader):\n",
    "        sensitive, test_target = attributes[:,20], attributes[:,9]\n",
    "        test_input = test_input.to(device)\n",
    "        test_target = test_target.to(device)\n",
    "        gt = test_target.detach().cpu().numpy()\n",
    "        sen = sensitive.detach().cpu().numpy()\n",
    "        test_gt.extend(gt)\n",
    "        sense_gt.extend(sen)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_pred_, _ , att= model(test_input, False)\n",
    "            test_pred.extend(torch.round(test_pred_.squeeze(1)).detach().cpu().numpy())\n",
    "            test_target = test_target.float().to(device)\n",
    "            test_loss = criterion(test_pred_, test_target.unsqueeze(1))\n",
    "            testing_loss += test_loss.item()\n",
    "    t_loss = testing_loss/len(test_data_loader)\n",
    "\n",
    "    for i in range(len(sense_gt)):\n",
    "        if sense_gt[i] == 0:\n",
    "            a0_predic.append(test_pred[i])\n",
    "            a0_gt.append(test_gt[i])\n",
    "        else:\n",
    "            a1_predic.append(test_pred[i])\n",
    "            a1_gt.append(test_gt[i])\n",
    "    a0_CM = confusion_matrix(a0_gt, a0_predic)    \n",
    "    a1_CM = confusion_matrix(a1_gt, a1_predic) \n",
    "    a0_dp = (a0_CM[1][1]+a0_CM[0][1])/(a0_CM[0][0]+a0_CM[0][1]+a0_CM[1][0]+a0_CM[1][1])\n",
    "    a1_dp = (a1_CM[1][1]+a1_CM[0][1])/(a1_CM[0][0]+a1_CM[0][1]+a1_CM[1][0]+a1_CM[1][1])\n",
    "    a0_TPR = a0_CM[1][1]/(a0_CM[1][1]+a0_CM[1][0])\n",
    "    a1_TPR = a1_CM[1][1]/(a1_CM[1][1]+a1_CM[1][0])\n",
    "    a0_FPR = a0_CM[0][1]/(a0_CM[0][1]+a0_CM[0][0])\n",
    "    a1_FPR = a1_CM[0][1]/(a1_CM[0][1]+a1_CM[0][0])\n",
    "    EOd = 0.5*(abs(a0_FPR-a1_FPR)+ abs(a0_TPR-a1_TPR))\n",
    "    test_acc = accuracy_score(test_gt, test_pred)\n",
    "    if p == True:\n",
    "        print('valid loss:',t_loss)\n",
    "        print('DP', abs(a0_dp - a1_dp))\n",
    "        print('EOP', abs(a0_TPR - a1_TPR))\n",
    "        print('EoD', EOd)\n",
    "        print('acc', test_acc)\n",
    "    return EOd\n",
    "\n",
    "\n",
    "def train_model(normalize):\n",
    "    configuration = ViTConfig(num_hidden_layers = 10, num_attention_heads = 8, \n",
    "                          intermediate_size = 768, image_size= 64, patch_size = 16)\n",
    "    vit = ViTModel(configuration)\n",
    "    configuration = vit.config\n",
    "    vit = vit.to(device)\n",
    "    model = VisionTransformer(vit, normalize)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    epoch = 50\n",
    "    criterion = nn.BCELoss()\n",
    "    fair_criterion = SupConLoss()\n",
    "\n",
    "    fair_optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay = 0.1)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    valid_loss =[]\n",
    "    valid_acc = []\n",
    "    fair_metric = 1\n",
    "    save_acc = 0\n",
    "    \n",
    "    \n",
    "    for epoches in range(epoch):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            for train_input, attributes in tepoch:\n",
    "                # Transfer data to GPU if possible. \n",
    "                train_input = train_input.to(device)\n",
    "                train_target = attributes[:,9]\n",
    "                train_target = train_target.float().to(device)\n",
    "                fair_optimizer.zero_grad()\n",
    "\n",
    "                outputs, value, _ = model(train_input, True)\n",
    "                value = value.unsqueeze(1)\n",
    "                fair_loss = fair_criterion(value, train_target.squeeze())\n",
    "                train_target = train_target.unsqueeze(1)\n",
    "                ut_loss = criterion(outputs, train_target)\n",
    "                loss =  ut_loss + fair_loss\n",
    "                tepoch.set_postfix(ul = ut_loss.item(),fl = fair_loss.item())  \n",
    "                loss.backward()\n",
    "                running_loss += loss.item()\n",
    "                fair_optimizer.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "                \n",
    "                all_preds.extend(torch.round(outputs.squeeze(1)).detach().cpu().numpy())\n",
    "                all_labels.extend(train_target.squeeze(1).cpu().numpy())\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch {epoches+1}/{epoch}, Loss: {running_loss/len(training_data_loader):.4f}, Accuracy: {train_accuracy * 100:.2f}%')\n",
    "        run_loss = running_loss/len(training_data_loader)\n",
    "        train_loss.append(run_loss) \n",
    "        train_acc.append(train_accuracy)\n",
    "        eod = test_epoch(model)\n",
    "        if fair_metric> eod:\n",
    "            fair_metric = eod\n",
    "            torch.save(model, 'best_celebA.pth')\n",
    "            print('saved')\n",
    "    return train_loss, train_acc, valid_loss, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae347081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0.000000 : 100%|██████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.45, ul=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 5.6699, Accuracy: 91.35%\n",
      "saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1.000000 : 100%|██████████████████████████████████████████| 540/540 [02:04<00:00,  4.32batch/s, fl=5.35, ul=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Loss: 5.5423, Accuracy: 93.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2.000000 : 100%|██████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.43, ul=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Loss: 5.5168, Accuracy: 94.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3.000000 : 100%|█████████████████████████████████████████| 540/540 [02:03<00:00,  4.37batch/s, fl=5.31, ul=0.0893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Loss: 5.4986, Accuracy: 94.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4.000000 : 100%|██████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.32, ul=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Loss: 5.4873, Accuracy: 94.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5.000000 : 100%|██████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.34, ul=0.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Loss: 5.4728, Accuracy: 95.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6.000000 : 100%|██████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.38, ul=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Loss: 5.4630, Accuracy: 95.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7.000000 : 100%|██████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.41, ul=0.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Loss: 5.4504, Accuracy: 95.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8.000000 : 100%|███████████████████████████████████████████| 540/540 [02:04<00:00,  4.35batch/s, fl=5.3, ul=0.103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Loss: 5.4422, Accuracy: 95.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9.000000 : 100%|██████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.28, ul=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 5.4272, Accuracy: 95.93%\n",
      "saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10.000000 : 100%|█████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.34, ul=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Loss: 5.4163, Accuracy: 96.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.31, ul=0.0882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Loss: 5.4031, Accuracy: 96.35%\n",
      "saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 12.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.32, ul=0.0805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Loss: 5.3864, Accuracy: 96.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 13.000000 : 100%|██████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.35, ul=0.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Loss: 5.3710, Accuracy: 96.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 14.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.25, ul=0.0912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Loss: 5.3564, Accuracy: 97.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 15.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.26, ul=0.0559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Loss: 5.3386, Accuracy: 97.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.32batch/s, fl=5.25, ul=0.0886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Loss: 5.3225, Accuracy: 97.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 17.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.19, ul=0.0522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Loss: 5.3096, Accuracy: 97.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 18.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.26, ul=0.0512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Loss: 5.2929, Accuracy: 97.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 19.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.25, ul=0.0748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: 5.2858, Accuracy: 98.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 20.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.16, ul=0.0582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Loss: 5.2684, Accuracy: 98.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 21.000000 : 100%|████████████████████████████████████████| 540/540 [02:05<00:00,  4.31batch/s, fl=5.21, ul=0.0863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Loss: 5.2591, Accuracy: 98.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 22.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.22, ul=0.0729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Loss: 5.2506, Accuracy: 98.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 23.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.15, ul=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Loss: 5.2391, Accuracy: 98.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 24.000000 : 100%|█████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.2, ul=0.0692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Loss: 5.2321, Accuracy: 98.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 25.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.16, ul=0.0373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Loss: 5.2294, Accuracy: 98.83%\n",
      "saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 26.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.32batch/s, fl=5.19, ul=0.0499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Loss: 5.2213, Accuracy: 98.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 27.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.35batch/s, fl=5.15, ul=0.0327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Loss: 5.2124, Accuracy: 99.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 28.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.16, ul=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Loss: 5.2160, Accuracy: 99.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 29.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.15, ul=0.0212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: 5.2098, Accuracy: 99.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 30.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.28, ul=0.0539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Loss: 5.2035, Accuracy: 99.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 31.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.11, ul=0.0201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Loss: 5.2029, Accuracy: 99.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 32.000000 : 100%|█████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.2, ul=0.0286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Loss: 5.1975, Accuracy: 99.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 33.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.32batch/s, fl=5.16, ul=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Loss: 5.1954, Accuracy: 99.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 34.000000 : 100%|███████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.15, ul=0.00744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Loss: 5.1922, Accuracy: 99.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 35.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.12, ul=0.0177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Loss: 5.1904, Accuracy: 99.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 36.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.19, ul=0.0319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Loss: 5.1893, Accuracy: 99.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 37.000000 : 100%|████████████████████████████████████████| 540/540 [02:05<00:00,  4.32batch/s, fl=5.25, ul=0.0416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Loss: 5.1859, Accuracy: 99.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 38.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.18, ul=0.0151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Loss: 5.1833, Accuracy: 99.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 39.000000 : 100%|█████████████████████████████████████████| 540/540 [02:04<00:00,  4.35batch/s, fl=5.2, ul=0.0333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: 5.1833, Accuracy: 99.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 40.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.35batch/s, fl=5.22, ul=0.0314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Loss: 5.1816, Accuracy: 99.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 41.000000 : 100%|█████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.2, ul=0.0267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Loss: 5.1813, Accuracy: 99.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 42.000000 : 100%|████████████████████████████████████████| 540/540 [02:03<00:00,  4.38batch/s, fl=5.19, ul=0.0253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Loss: 5.1815, Accuracy: 99.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 43.000000 : 100%|████████████████████████████████████████| 540/540 [02:03<00:00,  4.36batch/s, fl=5.19, ul=0.0273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Loss: 5.1764, Accuracy: 99.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 44.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.17, ul=0.0102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Loss: 5.1788, Accuracy: 99.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 45.000000 : 100%|██████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.17, ul=0.000937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Loss: 5.1790, Accuracy: 99.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 46.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.35batch/s, fl=5.17, ul=0.0214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Loss: 5.1726, Accuracy: 99.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 47.000000 : 100%|███████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.13, ul=0.00797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Loss: 5.1741, Accuracy: 99.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 48.000000 : 100%|███████████████████████████████████████| 540/540 [02:04<00:00,  4.34batch/s, fl=5.13, ul=0.00846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Loss: 5.1721, Accuracy: 99.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 49.000000 : 100%|████████████████████████████████████████| 540/540 [02:04<00:00,  4.33batch/s, fl=5.13, ul=0.0028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: 5.1713, Accuracy: 99.50%\n",
      "***********************************************\n",
      "energy results\n",
      "\n",
      "GPU Utilizations recorded every minute: [0, 27, 40, 38, 37, 21, 31, 39, 42, 11, 14, 16, 39, 5, 27, 15, 11, 39, 20, 26, 27, 25, 28, 19, 15, 11, 44, 41, 39, 40, 27, 1, 11, 40, 25, 36, 26, 16, 39, 21, 2, 24, 32, 34, 30, 6, 40, 29, 20, 39, 32, 38, 37, 37, 0, 26, 40, 15, 28, 10, 40, 19, 0, 40, 11, 25, 27, 24, 39, 40, 19, 22, 39, 11, 41, 1, 39, 39, 40, 24, 11, 33, 25, 38, 40, 32, 14, 27, 24, 11, 39, 39, 18, 38, 0, 35, 11, 39, 39, 40, 40, 23, 40, 28, 16, 11, 37, 20, 40, 5, 12, 8, 19, 39, 19]\n",
      "Average hourly GPU Utilizations: [25.8, 24.166666666666668]\n",
      "\n",
      "Total training time: 6875.14 seconds.\n",
      "testing\n"
     ]
    }
   ],
   "source": [
    "import subprocess  \n",
    "import threading  \n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def compute_average_hourly_utilization(minute_utilizations):\n",
    "    padding_length = (60 - len(minute_utilizations) % 60) % 60 \n",
    "    padded_utilizations = minute_utilizations + [0] * padding_length\n",
    "    num_hours = len(padded_utilizations) // 60\n",
    "    hourly_averages = [sum(padded_utilizations[i*60:(i+1)*60])/60.0 for i in range(num_hours)]\n",
    "\n",
    "    return hourly_averages\n",
    "\n",
    "def get_gpu_utilization(device_id):\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=utilization.gpu\", \"--format=csv,noheader,nounits\"]\n",
    "        ).decode(\"utf-8\").strip().split('\\n')\n",
    "        return int(result[device_id])# please change the device_id accordingly\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying GPU utilization: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def track_gpu_utilization_periodically(device_id):\n",
    "    if not stop_tracking:\n",
    "        utilization = get_gpu_utilization(device_id)\n",
    "        if utilization is not None:\n",
    "            utilization_list.append(utilization)\n",
    "        threading.Timer(60, track_gpu_utilization_periodically, args=[device_id]).start()\n",
    "\n",
    "\n",
    "utilization_list = []\n",
    "stop_tracking = False\n",
    "\n",
    "# Start the periodic GPU tracking\n",
    "track_gpu_utilization_periodically(device_id)\n",
    "start_time = time.time()    \n",
    "    \n",
    "normalize = True\n",
    "seed_everything(0)    \n",
    "tl,ta,vl,va = train_model(normalize)\n",
    "\n",
    "stop_tracking = True\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Print GPU utilizations\n",
    "print(\"***********************************************\")\n",
    "print(\"energy results\")\n",
    "print(\"\\nGPU Utilizations recorded every minute:\", utilization_list)\n",
    "\n",
    "with open('utilization_data.pkl', 'wb') as f:\n",
    "    pickle.dump(utilization_list, f)\n",
    "    \n",
    "average_hourly_utilizations = compute_average_hourly_utilization(utilization_list)\n",
    "print(\"Average hourly GPU Utilizations:\", average_hourly_utilizations)\n",
    "print(f\"\\nTotal training time: {total_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "## test\n",
    "print('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b9ac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing result\n",
      "valid loss: 0.20273513585711136\n",
      "DP 0.18146682767179192\n",
      "EOP 0.3697132616487456\n",
      "EoD 0.20552938088417697\n",
      "acc 0.9389339745516482\n"
     ]
    }
   ],
   "source": [
    "print('Testing result')\n",
    "configuration = ViTConfig(num_hidden_layers = 10, num_attention_heads = 8, \n",
    "                          intermediate_size = 768, image_size= 64, patch_size = 16)\n",
    "vit = ViTModel(configuration)\n",
    "model = VisionTransformer(vit, normalize)\n",
    "model = torch.load('best_celebA.pth')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "_ = test_epoch(model, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
