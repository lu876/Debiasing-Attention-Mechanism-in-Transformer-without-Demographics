{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec1e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenyu/miniconda3/envs/DLcourse/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#step 1 import image\n",
    "%matplotlib inline\n",
    "import torchvision.datasets\n",
    "import math\n",
    "import torchvision.transforms as tvt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTModel\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "image_size = 224\n",
    "batch_size = 128\n",
    "dataset = torchvision.datasets.CelebA(\"../../celeba/datasets/\",split='train', transform=tvt.Compose([\n",
    "                                  tvt.Resize((image_size,image_size)),\n",
    "                                  tvt.ToTensor(),\n",
    "                                  tvt.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                std=[0.5, 0.5, 0.5])\n",
    "                              ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.CelebA(\"../../celeba/datasets/\",split='test', transform=tvt.Compose([\n",
    "                                  tvt.Resize((image_size,image_size)),\n",
    "                                  tvt.ToTensor(),\n",
    "                                  tvt.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                std=[0.5, 0.5, 0.5])\n",
    "                              ]))\n",
    "\n",
    "training_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e6c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_value_patch(attention, value, k):\n",
    "    \"\"\"\n",
    "    Get top-k attention values based on average attention weights across heads.\n",
    "\n",
    "    Parameters:\n",
    "    - attention (tensor): Shape [Batch, Head, token]\n",
    "    - value (tensor): Shape [Batch, token, dim]\n",
    "    - k (int): Number of top attention values to select (default is 2)\n",
    "\n",
    "    Returns:\n",
    "    - top_k_values (tensor): Shape [Batch, k, dim]\n",
    "    \"\"\"\n",
    "\n",
    "    # Average attention across the head dimension.\n",
    "    avg_attention = attention.mean(dim=1)\n",
    "\n",
    "    # Get the top-k attention indices.\n",
    "    _, top_k_indices = avg_attention.topk(k, dim=-1)\n",
    "\n",
    "    # Gather the top-k values.\n",
    "    batch_size, _ = top_k_indices.shape\n",
    "    batch_indices = torch.arange(batch_size)[:, None].to(top_k_indices.device)\n",
    "    top_k_values = value[batch_indices, top_k_indices].view(batch_size, k, -1)\n",
    "\n",
    "    return top_k_values\n",
    "\n",
    "class Last_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Last_Attention, self).__init__()\n",
    "        self.p_dim = 5\n",
    "        self.emb_size = 384\n",
    "        self.head = 6\n",
    "        self.head_dim = self.emb_size //self.head\n",
    "        self.Q = nn.Linear(384,384)\n",
    "        self.K = nn.Linear(384,384)\n",
    "        self.V = nn.Linear(384,384)\n",
    "        self.projection = nn.Linear(384, 384)\n",
    "        self.soft_max = nn.Softmax(dim=-1)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.p_dim*384, 256, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128, bias=False),\n",
    "        )\n",
    "        self.momentum = 0.1\n",
    "        self.register_buffer('running_mean_q', torch.zeros(1,6,197,64))\n",
    "        self.register_buffer('running_std_q', torch.ones(1,6,197,64))\n",
    "        self.register_buffer('running_mean_k', torch.zeros(1,6,197,64))\n",
    "        self.register_buffer('running_std_k', torch.ones(1,6,197,64))\n",
    "        \n",
    "    def forward(self, x, training):\n",
    "        B, N, C = x.shape\n",
    "        origin_k = self.K(x)\n",
    "        origin_q = self.Q(x)\n",
    "        origin_v = self.V(x)\n",
    "        \n",
    "        q = origin_q.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        k = origin_k.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        v = origin_v.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        \n",
    "        self.running_mean_q = self.running_mean_q.detach()\n",
    "        self.running_std_q = self.running_std_q.detach()\n",
    "        self.running_mean_k = self.running_mean_k.detach()\n",
    "        self.running_std_k = self.running_std_k.detach()\n",
    "\n",
    "        if training:\n",
    "            with torch.no_grad():\n",
    "                q_mean, q_std = torch.mean(q, 0, keepdim=True), torch.std(q, 0, keepdim=True)\n",
    "                k_mean, k_std = torch.mean(k, 0, keepdim=True), torch.std(k, 0, keepdim=True)  \n",
    "\n",
    "                self.running_mean_q = (1 - self.momentum) * self.running_mean_q.to(device) + self.momentum * q_mean\n",
    "                self.running_std_q = (1 - self.momentum) * self.running_std_q.to(device) + self.momentum * q_std\n",
    "                self.running_mean_k = (1 - self.momentum) * self.running_mean_k.to(device) + self.momentum * k_mean\n",
    "                self.running_std_k = (1 - self.momentum) * self.running_std_k.to(device) + self.momentum * k_std\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_mean = self.running_mean_q\n",
    "                q_std = self.running_std_q\n",
    "                k_mean = self.running_mean_k\n",
    "                k_std = self.running_std_k\n",
    "        \n",
    "        q = (q - q_mean) \n",
    "        q = torch.abs(q)\n",
    "        k = (k - k_mean) \n",
    "        k = torch.abs(k)\n",
    "        \n",
    "        attention = (q @ k.transpose(-2,-1))* (self.head_dim ** (-0.5))\n",
    "        atten = self.soft_max(attention)\n",
    "        out = (atten @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.projection(out)\n",
    "        attentions = atten[:,:, 0, :]\n",
    "        v = v.transpose(1, 2).reshape(B, N, C)\n",
    "        mst_val = choose_value_patch(attentions, v, self.p_dim)\n",
    "        mst_val = mst_val.reshape(B, -1)\n",
    "        \n",
    "        mst_val = self.projector(mst_val)\n",
    "        z = F.normalize(mst_val, dim=1)\n",
    "        return out, z, atten\n",
    "\n",
    "    \n",
    "class Last_ATBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = 384\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = Last_Attention()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(384, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(384, 384)          \n",
    "        )\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x, training):\n",
    "        identity = x\n",
    "        x = self.norm(x)\n",
    "        x, vz, att = self.attention(x, training)\n",
    "        x += identity\n",
    "        res = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x += res\n",
    "        return x, vz, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f37b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.last_layer = Last_ATBlock()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(384, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),     \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, training):\n",
    "        x, vz, att = self.last_layer(x, training)\n",
    "        x = x[:,0]\n",
    "        y = self.seq(x)\n",
    "        return y, vz, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10171b2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "epoch 0.000000 : 100%|████████████████████████████████████████| 1271/1271 [08:36<00:00,  2.46batch/s, fl=4.66, ul=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8483870967741935\n",
      "male TPR 0.25555555555555554\n",
      "DP 0.19888178733681178\n",
      "EOP 0.592831541218638\n",
      "EoD 0.3175224738485421\n",
      "acc 0.950806532411582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1.000000 : 100%|███████████████████████████████████████| 1271/1271 [09:24<00:00,  2.25batch/s, fl=4.76, ul=0.0865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8370967741935483\n",
      "male TPR 0.3055555555555556\n",
      "DP 0.1909717464369308\n",
      "EOP 0.5315412186379928\n",
      "EoD 0.2840977375786473\n",
      "acc 0.952459673379421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2.000000 : 100%|███████████████████████████████████████| 1271/1271 [09:54<00:00,  2.14batch/s, fl=4.71, ul=0.0984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8641129032258065\n",
      "male TPR 0.37777777777777777\n",
      "DP 0.19860538544469286\n",
      "EOP 0.4863351254480287\n",
      "EoD 0.2640562668764869\n",
      "acc 0.9528103396453261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3.000000 : 100%|███████████████████████████████████████| 1271/1271 [09:56<00:00,  2.13batch/s, fl=4.67, ul=0.0666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.7810483870967742\n",
      "male TPR 0.21666666666666667\n",
      "DP 0.17405312692869987\n",
      "EOP 0.5643817204301075\n",
      "EoD 0.2954877582307912\n",
      "acc 0.9514577697625488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4.000000 : 100%|████████████████████████████████████████| 1271/1271 [09:54<00:00,  2.14batch/s, fl=4.73, ul=0.117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.7923387096774194\n",
      "male TPR 0.20555555555555555\n",
      "DP 0.1789043209280398\n",
      "EOP 0.5867831541218638\n",
      "EoD 0.30817876621248963\n",
      "acc 0.9509568179541128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5.000000 : 100%|████████████████████████████████████████| 1271/1271 [07:42<00:00,  2.75batch/s, fl=4.67, ul=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8451612903225807\n",
      "male TPR 0.35\n",
      "DP 0.1916220783049439\n",
      "EOP 0.4951612903225807\n",
      "EoD 0.26625483133687217\n",
      "acc 0.9511071034966436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6.000000 : 100%|███████████████████████████████████████| 1271/1271 [08:47<00:00,  2.41batch/s, fl=4.66, ul=0.0742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.867741935483871\n",
      "male TPR 0.42777777777777776\n",
      "DP 0.19714814759348792\n",
      "EOP 0.43996415770609326\n",
      "EoD 0.24074957804065406\n",
      "acc 0.94980462879471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7.000000 : 100%|████████████████████████████████████████| 1271/1271 [09:22<00:00,  2.26batch/s, fl=4.69, ul=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8169354838709677\n",
      "male TPR 0.34444444444444444\n",
      "DP 0.18310565932239095\n",
      "EOP 0.47249103942652326\n",
      "EoD 0.2529781436669092\n",
      "acc 0.9502554854223024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8.000000 : 100%|████████████████████████████████████████| 1271/1271 [10:30<00:00,  2.02batch/s, fl=4.69, ul=0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8149193548387097\n",
      "male TPR 0.26666666666666666\n",
      "DP 0.18615424010885043\n",
      "EOP 0.5482526881720431\n",
      "EoD 0.291784202172808\n",
      "acc 0.9492034866245868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9.000000 : 100%|████████████████████████████████████████| 1271/1271 [14:05<00:00,  1.50batch/s, fl=4.6, ul=0.0641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.7435483870967742\n",
      "male TPR 0.21666666666666667\n",
      "DP 0.16393445329582215\n",
      "EOP 0.5268817204301075\n",
      "EoD 0.27548267538441223\n",
      "acc 0.9454964432421601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10.000000 : 100%|██████████████████████████████████████| 1271/1271 [14:42<00:00,  1.44batch/s, fl=4.68, ul=0.0303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8213709677419355\n",
      "male TPR 0.4722222222222222\n",
      "DP 0.18133833190780754\n",
      "EOP 0.3491487455197133\n",
      "EoD 0.19229504137916428\n",
      "acc 0.9448953010720369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11.000000 : 100%|██████████████████████████████████████| 1271/1271 [15:09<00:00,  1.40batch/s, fl=4.58, ul=0.0737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8358870967741936\n",
      "male TPR 0.49444444444444446\n",
      "DP 0.18288796478574698\n",
      "EOP 0.34144265232974913\n",
      "EoD 0.18852179711138548\n",
      "acc 0.9419897805831079\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    epoch = 12\n",
    "    DINO = ViTModel.from_pretrained('facebook/dino-vits16').to(device)\n",
    "    model = VisionTransformer()\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    fair_criterion = SupConLoss()\n",
    "    for name, param in DINO.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    \n",
    "\n",
    "    \n",
    "    for epoches in range(epoch):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            for train_input, attributes in tepoch:\n",
    "                # Transfer data to GPU if possible. \n",
    "                train_input = train_input.to(device)\n",
    "                sensitive, train_target = attributes[:,20], attributes[:,9]\n",
    "               \n",
    "                train_target = train_target.float().to(device)\n",
    "                #train_target = train_target.unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    y = DINO(train_input)\n",
    "                outputs, value, _ = model(y.last_hidden_state, True)\n",
    "                value = value.unsqueeze(1)\n",
    "                fair_loss = fair_criterion(value, train_target.squeeze())\n",
    "                train_target = train_target.unsqueeze(1)\n",
    "                ut_loss = criterion(outputs, train_target)\n",
    "                \n",
    "                loss =  ut_loss + fair_loss\n",
    "                tepoch.set_postfix(ul = ut_loss.item(),fl = fair_loss.item())  \n",
    "                    \n",
    "                    \n",
    "                loss.backward()\n",
    "                #logger_learner.add_values(logging_dict)\n",
    "                optimizer.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "                \n",
    "\n",
    "        # Reset the dataloader if out of data.\n",
    "        #model.load_state_dict(torch.load(PATH), False)\n",
    "        \n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate on test set.\n",
    "        for step, (test_input, attributes) in enumerate(test_data_loader):\n",
    "            sensitive, test_target = attributes[:,20], attributes[:,9]\n",
    "            test_input = test_input.to(device)\n",
    "            test_target = test_target.to(device)\n",
    "\n",
    "            gt = test_target.detach().cpu().numpy()\n",
    "            sen = sensitive.detach().cpu().numpy()\n",
    "            test_gt.extend(gt)\n",
    "            sense_gt.extend(sen)\n",
    "\n",
    "            # Todo: split according to sensitive attribute\n",
    "            # Todo: combine all batch togather\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y = DINO(test_input)\n",
    "                test_pred_, _, _ = model(y.last_hidden_state, False)\n",
    "                test_pred.extend(torch.round(test_pred_.squeeze(1)).detach().cpu().numpy())\n",
    "\n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "        female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "        male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "        female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "        male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0])\n",
    "        female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "        male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "\n",
    "        print('Female TPR', female_TPR)\n",
    "        print('male TPR', male_TPR)\n",
    "        print('DP',abs(female_dp - male_dp))\n",
    "        print('EOP', abs(female_TPR - male_TPR))\n",
    "        print('EoD',0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR)))\n",
    "        print('acc', accuracy_score(test_gt, test_pred))\n",
    "\n",
    "\n",
    "seed_everything(0)    \n",
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
