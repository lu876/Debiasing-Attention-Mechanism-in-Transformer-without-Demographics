{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1 import image\n",
    "%matplotlib inline\n",
    "import torchvision.datasets\n",
    "import math\n",
    "import torchvision.transforms as tvt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tfms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTConfig, ViTModel\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "\"\"\"\n",
    "data_root = \"../celeba/datasets\"\n",
    "\n",
    "base_url = \"https://graal.ift.ulaval.ca/public/celeba/\"\n",
    "\n",
    "file_list = [\n",
    "    \"img_align_celeba.zip\",\n",
    "    \"list_attr_celeba.txt\",\n",
    "    \"identity_CelebA.txt\",\n",
    "    \"list_bbox_celeba.txt\",\n",
    "    \"list_landmarks_align_celeba.txt\",\n",
    "    \"list_eval_partition.txt\",\n",
    "]\n",
    "\n",
    "# Path to folder with the dataset\n",
    "dataset_folder = f\"{data_root}/celeba\"\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "for file in file_list:\n",
    "    url = f\"{base_url}/{file}\"\n",
    "    if not os.path.exists(f\"{dataset_folder}/{file}\"):\n",
    "        wget.download(url, f\"{dataset_folder}/{file}\")\n",
    "\n",
    "with zipfile.ZipFile(f\"{dataset_folder}/img_align_celeba.zip\", \"r\") as ziphandler:\n",
    "    ziphandler.extractall(dataset_folder)\n",
    "\"\"\"\n",
    "\n",
    "image_size = 64\n",
    "batch_size = 256\n",
    "dataset = torchvision.datasets.CelebA(\"../celeba/datasets/\",split='train', transform=tvt.Compose([\n",
    "                                  tvt.Resize((image_size,image_size)),\n",
    "                                  tvt.ToTensor(),\n",
    "                                  tvt.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                std=[0.5, 0.5, 0.5])\n",
    "                              ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.CelebA(\"../celeba/datasets/\",split='test', transform=tvt.Compose([\n",
    "                                  tvt.Resize((image_size,image_size)),\n",
    "                                  tvt.ToTensor(),\n",
    "                                  tvt.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                std=[0.5, 0.5, 0.5])\n",
    "                              ]))\n",
    "\n",
    "training_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fcdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTModel\n",
    "configuration = ViTConfig(num_hidden_layers = 8, num_attention_heads = 8, \n",
    "                          intermediate_size = 768, image_size= 64, patch_size = 16)\n",
    "model = ViTModel(configuration)\n",
    "configuration = model.config\n",
    "t = iter(test_data_loader)\n",
    "img, label = next(t)\n",
    "img\n",
    "y = model(img)\n",
    "y.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_value_patch(atten, value, p_dim):\n",
    "    # input insturction: \n",
    "    # atten: shape: Batch, Head, Patch\n",
    "    # value: Batch, Head, Patch, Dim\n",
    "    # Output: Batch, Head, Selct_Patch, dim\n",
    "    atten = atten[:,:,1:]\n",
    "    top_k_values, top_k_indices = torch.topk(atten, k=p_dim, dim=2, sorted=False)\n",
    "    #top_k_indices : Batch, Head, Select_patch\n",
    "    output = torch.gather(value, 2, top_k_indices.unsqueeze(-1).expand(-1,-1,-1,value.size(-1)))\n",
    "    return output\n",
    "    \n",
    "class Last_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Last_Attention, self).__init__()\n",
    "        self.p_dim = 3\n",
    "        self.emb_size = 768\n",
    "        self.head = 8\n",
    "        self.temperature = 1\n",
    "        self.head_dim = self.emb_size //self.head\n",
    "        self.Q = nn.Linear(768,768)\n",
    "        self.K = nn.Linear(768,768)\n",
    "        self.V = nn.Linear(768,768)\n",
    "        self.projection = nn.Linear(768, 768)\n",
    "        self.soft_max = nn.Softmax(dim=-1)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.p_dim*768, 256, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128, bias=False),\n",
    "        )\n",
    "        self.momentum = 0.05\n",
    "        self.register_buffer('running_mean_q', torch.zeros(1,8,17,96))\n",
    "        self.register_buffer('running_std_q', torch.ones(1,8,17,96))\n",
    "        self.register_buffer('running_mean_k', torch.zeros(1,8,17,96))\n",
    "        self.register_buffer('running_std_k', torch.ones(1,8,17,96))\n",
    "\n",
    "    def register_buffer(self, name, tensor):\n",
    "        setattr(self, name, tensor)\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        B, N, C = x.shape\n",
    "        origin_k = self.K(x)\n",
    "        origin_q = self.Q(x)\n",
    "        origin_v = self.V(x)\n",
    "        self.running_mean_q = self.running_mean_q.detach()\n",
    "        self.running_std_q = self.running_std_q.detach()\n",
    "        self.running_mean_k = self.running_mean_k.detach()\n",
    "        self.running_std_k = self.running_std_k.detach()\n",
    "        q = origin_q.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        k = origin_k.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        \n",
    "        if training:\n",
    "            with torch.no_grad():\n",
    "                q_mean, q_std = torch.mean(q, 0, keepdim=True), torch.std(q, 0, keepdim=True)\n",
    "                k_mean, k_std = torch.mean(k, 0, keepdim=True), torch.std(k, 0, keepdim=True)  \n",
    "\n",
    "                self.running_mean_q = (1 - self.momentum) * self.running_mean_q.to(device) + self.momentum * q_mean\n",
    "                self.running_std_q = (1 - self.momentum) * self.running_std_q.to(device) + self.momentum * q_std\n",
    "                self.running_mean_k = (1 - self.momentum) * self.running_mean_k.to(device) + self.momentum * k_mean\n",
    "                self.running_std_k = (1 - self.momentum) * self.running_std_k.to(device) + self.momentum * k_std\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_mean = self.running_mean_q\n",
    "                q_std = self.running_std_q\n",
    "                k_mean = self.running_mean_k\n",
    "                k_std = self.running_std_k\n",
    "        \n",
    "        q = (q - q_mean) /q_std\n",
    "        q = torch.abs(q)\n",
    "        k = (k - k_mean) /k_std\n",
    "        k = torch.abs(k)\n",
    "        \n",
    "        v = origin_v.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        attention = (q @ k.transpose(-2,-1))* (self.head_dim ** (-0.5))    \n",
    "        atten = self.soft_max(attention/self.temperature)\n",
    "        out = (atten @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.projection(out)\n",
    "        attentions = atten[:,:, 0, :]\n",
    "        mst_val = choose_value_patch(attentions, v, self.p_dim)\n",
    "        mst_val = mst_val.reshape(B, -1)\n",
    "        mst_val = self.projector(mst_val)\n",
    "        z = F.normalize(mst_val, dim=1)\n",
    "        return out, z\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "class Last_ATBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dim = 768\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = Last_Attention()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768)          \n",
    "        )\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        identity = x\n",
    "        x = self.norm(x)\n",
    "        x, vz = self.attention(x, training)\n",
    "        x += identity\n",
    "        res = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x += res\n",
    "        return x, vz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, vit):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.vit = vit\n",
    "        self.last_encoder = Last_ATBlock()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 1),     \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        z = self.vit(x)\n",
    "        m = z.last_hidden_state\n",
    "        m, vz = self.last_encoder(m, training)\n",
    "        g = m[:,0]\n",
    "        y = self.seq(g)\n",
    "        return y, vz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc3e46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss    \n",
    "\n",
    "    \n",
    "\n",
    "def train_model():\n",
    "    configuration = ViTConfig(num_hidden_layers = 7, num_attention_heads = 8, \n",
    "                          intermediate_size = 768, image_size= 64, patch_size = 16)\n",
    "    vit = ViTModel(configuration)\n",
    "    configuration = vit.config\n",
    "    vit = vit.to(device)\n",
    "    model = VisionTransformer(vit)\n",
    "    model = model.to(device)\n",
    "    epoch = 20\n",
    "    criterion = nn.BCELoss()\n",
    "    fair_criterion = SupConLoss()\n",
    "    fair_optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    \n",
    "    for epoches in range(epoch):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            for train_input, attributes in tepoch:\n",
    "                # Transfer data to GPU if possible. \n",
    "                train_input = train_input.to(device)\n",
    "                train_target = attributes[:,2]\n",
    "                train_target = train_target.float().to(device)\n",
    "                fair_optimizer.zero_grad()\n",
    "\n",
    "                # Learner update step.\n",
    "                #if fairness processorigin_v\n",
    "                outputs, value = model(train_input)\n",
    "                value = value.unsqueeze(1)\n",
    "                fair_loss = fair_criterion(value, train_target.squeeze())\n",
    "                train_target = train_target.unsqueeze(1)\n",
    "                ut_loss = criterion(outputs, train_target)\n",
    "                loss =  ut_loss + fair_loss\n",
    "                tepoch.set_postfix(ul = ut_loss.item(),fl = fair_loss.item())  \n",
    "                loss.backward()\n",
    "                fair_optimizer.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "\n",
    "    # Evaluate on validation\n",
    "        for step, (test_input, attributes) in enumerate(test_data_loader):\n",
    "            sensitive, test_target = attributes[:,20], attributes[:,2]\n",
    "            test_input = test_input.to(device)\n",
    "            test_target = test_target.to(device)\n",
    "\n",
    "            gt = test_target.detach().cpu().numpy()\n",
    "            sen = sensitive.detach().cpu().numpy()\n",
    "            test_gt.extend(gt)\n",
    "            sense_gt.extend(sen)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_pred_, _ = model(test_input, False)\n",
    "                test_pred.extend(torch.round(test_pred_.squeeze(1)).detach().cpu().numpy())\n",
    "\n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "        female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "        male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "        female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "        male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0])\n",
    "        female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "        male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "\n",
    "        print('Female TPR', female_TPR)\n",
    "        print('male TPR', male_TPR)\n",
    "        print('DP',abs(female_dp - male_dp))\n",
    "        print('EOP', abs(female_TPR - male_TPR))\n",
    "        print('EoD',0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR)))\n",
    "        print('acc', accuracy_score(test_gt, test_pred))\n",
    "        print('Trade off',accuracy_score(test_gt, test_pred)*(1-0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR))) )\n",
    "        \n",
    "\n",
    "seed_everything(4096)    \n",
    "train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
