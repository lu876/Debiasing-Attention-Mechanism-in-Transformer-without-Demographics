{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f822e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n##################################################################################\\nimport os\\nimport sys\\nimport numpy as np\\nimport random\\nimport string\\nimport pandas as pd\\n\\n################ Paths and other configs - Set these #################################\\n\\ndata_dir = 'MultiNLI'\\nglue_dir = 'MultiNLI'\\n\\ntype_of_split = 'random'\\nassert type_of_split in ['preset', 'random']\\n# If 'preset', use the official train/val/test MultiNLI split\\n# If 'random', randomly split 50%/20%/30% of the data to train/val/test\\n\\n######################################################################################\\n\\ndef tokenize(s):\\n    s = s.translate(str.maketrans('', '', string.punctuation))\\n    s = s.lower()\\n    s = s.split(' ')\\n    return s\\n\\n### Read in data and assign train/val/test splits\\ntrain_df = pd.read_json(\\n    os.path.join(\\n        data_dir,\\n        'multinli_1.0_train.jsonl'),\\n    lines=True)\\n\\nval_df = pd.read_json(\\n    os.path.join(\\n        data_dir,\\n        'multinli_1.0_dev_matched.jsonl'),\\n    lines=True)\\n\\ntest_df = pd.read_json(\\n    os.path.join(\\n        data_dir,\\n        'multinli_1.0_dev_mismatched.jsonl'),\\n    lines=True)\\n\\nsplit_dict = {\\n    'train': 0,\\n    'val': 1,\\n    'test': 2\\n}\\n\\nif type_of_split == 'preset':\\n    train_df['split'] = split_dict['train']\\n    val_df['split'] = split_dict['val']\\n    test_df['split'] = split_dict['test']\\n    df = pd.concat([train_df, val_df, test_df], ignore_index=True)\\n\\nelif type_of_split == 'random':\\n    val_frac = 0.2\\n    test_frac = 0.3\\n\\n    df = pd.concat([train_df, val_df, test_df], ignore_index=True)\\n    n = len(df)\\n    n_val = int(val_frac * n)\\n    n_test = int(test_frac * n)\\n    n_train = n - n_val - n_test\\n    splits = np.array([split_dict['train']] * n_train + [split_dict['val']] * n_val + [split_dict['test']] * n_test)\\n    np.random.shuffle(splits)\\n    df['split'] = splits\\n\\n### Assign labels\\ndf = df.loc[df['gold_label'] != '-', :]\\nprint(f'Total number of examples: {len(df)}')\\nfor k, v in split_dict.items():\\n    print(k, np.mean(df['split'] == v))\\n\\nlabel_dict = {\\n    'contradiction': 0,\\n    'entailment': 1,\\n    'neutral': 2\\n}\\nfor k, v in label_dict.items():\\n    idx = df.loc[:, 'gold_label'] == k\\n    df.loc[idx, 'gold_label'] = v\\n\\n### Assign spurious attribute (negation words)\\nnegation_words = ['nobody', 'no', 'never', 'nothing'] # Taken from https://arxiv.org/pdf/1803.02324.pdf\\n\\ndf['sentence2_has_negation'] = [False] * len(df)\\n\\nfor negation_word in negation_words:\\n    df['sentence2_has_negation'] |= [negation_word in tokenize(sentence) for sentence in df['sentence2']]\\n\\ndf['sentence2_has_negation'] = df['sentence2_has_negation'].astype(int)\\n\\n## Write to disk\\ndf = df[['gold_label', 'sentence2_has_negation', 'split']]\\ndf.to_csv(os.path.join(data_dir, f'metadata_{type_of_split}.csv'))\\nprint('Done')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run once is enough!!\n",
    "\"\"\"\n",
    "##################################################################################\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "################ Paths and other configs - Set these #################################\n",
    "\n",
    "data_dir = 'MultiNLI'\n",
    "glue_dir = 'MultiNLI'\n",
    "\n",
    "type_of_split = 'random'\n",
    "assert type_of_split in ['preset', 'random']\n",
    "# If 'preset', use the official train/val/test MultiNLI split\n",
    "# If 'random', randomly split 50%/20%/30% of the data to train/val/test\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "def tokenize(s):\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = s.lower()\n",
    "    s = s.split(' ')\n",
    "    return s\n",
    "\n",
    "### Read in data and assign train/val/test splits\n",
    "train_df = pd.read_json(\n",
    "    os.path.join(\n",
    "        data_dir,\n",
    "        'multinli_1.0_train.jsonl'),\n",
    "    lines=True)\n",
    "\n",
    "val_df = pd.read_json(\n",
    "    os.path.join(\n",
    "        data_dir,\n",
    "        'multinli_1.0_dev_matched.jsonl'),\n",
    "    lines=True)\n",
    "\n",
    "test_df = pd.read_json(\n",
    "    os.path.join(\n",
    "        data_dir,\n",
    "        'multinli_1.0_dev_mismatched.jsonl'),\n",
    "    lines=True)\n",
    "\n",
    "split_dict = {\n",
    "    'train': 0,\n",
    "    'val': 1,\n",
    "    'test': 2\n",
    "}\n",
    "\n",
    "if type_of_split == 'preset':\n",
    "    train_df['split'] = split_dict['train']\n",
    "    val_df['split'] = split_dict['val']\n",
    "    test_df['split'] = split_dict['test']\n",
    "    df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "elif type_of_split == 'random':\n",
    "    val_frac = 0.2\n",
    "    test_frac = 0.3\n",
    "\n",
    "    df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "    n = len(df)\n",
    "    n_val = int(val_frac * n)\n",
    "    n_test = int(test_frac * n)\n",
    "    n_train = n - n_val - n_test\n",
    "    splits = np.array([split_dict['train']] * n_train + [split_dict['val']] * n_val + [split_dict['test']] * n_test)\n",
    "    np.random.shuffle(splits)\n",
    "    df['split'] = splits\n",
    "\n",
    "### Assign labels\n",
    "df = df.loc[df['gold_label'] != '-', :]\n",
    "print(f'Total number of examples: {len(df)}')\n",
    "for k, v in split_dict.items():\n",
    "    print(k, np.mean(df['split'] == v))\n",
    "\n",
    "label_dict = {\n",
    "    'contradiction': 0,\n",
    "    'entailment': 1,\n",
    "    'neutral': 2\n",
    "}\n",
    "for k, v in label_dict.items():\n",
    "    idx = df.loc[:, 'gold_label'] == k\n",
    "    df.loc[idx, 'gold_label'] = v\n",
    "\n",
    "### Assign spurious attribute (negation words)\n",
    "negation_words = ['nobody', 'no', 'never', 'nothing'] # Taken from https://arxiv.org/pdf/1803.02324.pdf\n",
    "\n",
    "df['sentence2_has_negation'] = [False] * len(df)\n",
    "\n",
    "for negation_word in negation_words:\n",
    "    df['sentence2_has_negation'] |= [negation_word in tokenize(sentence) for sentence in df['sentence2']]\n",
    "\n",
    "df['sentence2_has_negation'] = df['sentence2_has_negation'].astype(int)\n",
    "\n",
    "## Write to disk\n",
    "df = df[['gold_label', 'sentence2_has_negation', 'split']]\n",
    "df.to_csv(os.path.join(data_dir, f'metadata_{type_of_split}.csv'))\n",
    "print('Done')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7d7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenyu/miniconda3/envs/DLcourse/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382945\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, Subset    \n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "class MultiNLIDataset():\n",
    "    \"\"\"\n",
    "    MultiNLI dataset.\n",
    "    label_dict = {\n",
    "        'contradiction': 0,\n",
    "        'entailment': 1,\n",
    "        'neutral': 2\n",
    "    }\n",
    "    # Negation words taken from https://arxiv.org/pdf/1803.02324.pdf\n",
    "    negation_words = ['nobody', 'no', 'never', 'nothing']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir,\n",
    "                 target_name, confounder_names,\n",
    "                 augment_data=False,\n",
    "                 model_type=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_name = target_name\n",
    "        self.confounder_names = confounder_names\n",
    "        self.model_type = model_type\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        assert len(confounder_names) == 1\n",
    "        assert confounder_names[0] == 'sentence2_has_negation'\n",
    "        assert target_name in ['gold_label_preset', 'gold_label_random']\n",
    "        assert augment_data == False\n",
    "\n",
    "        self.data_dir = os.path.join(\n",
    "            self.root_dir)\n",
    "        self.glue_dir = os.path.join(\n",
    "            self.root_dir)\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.data_dir} does not exist yet. Please generate the dataset first.')\n",
    "        if not os.path.exists(self.glue_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.glue_dir} does not exist yet. Please generate the dataset first.')\n",
    "\n",
    "        # Read in metadata\n",
    "        type_of_split = target_name.split('_')[-1]\n",
    "        self.metadata_df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                self.data_dir,\n",
    "                f'metadata_{type_of_split}.csv'),\n",
    "            index_col=0)\n",
    "\n",
    "        # Get the y values\n",
    "        # gold_label is hardcoded\n",
    "        self.y_array = self.metadata_df['gold_label'].values\n",
    "        self.n_classes = len(np.unique(self.y_array))\n",
    "\n",
    "        self.confounder_array = self.metadata_df[confounder_names[0]].values\n",
    "        print(np.sum(self.confounder_array==0))\n",
    "        self.n_confounders = len(confounder_names)\n",
    "\n",
    "        # Map to groups\n",
    "        self.n_groups = len(np.unique(self.confounder_array)) * self.n_classes\n",
    "        self.group_array = (self.y_array*(self.n_groups/self.n_classes) + self.confounder_array).astype('int')\n",
    "\n",
    "        # Extract splits\n",
    "        self.split_array = self.metadata_df['split'].values\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "\n",
    "        # Load features\n",
    "        self.features_array = []\n",
    "        for feature_file in [\n",
    "            'cached_train_bert-base-uncased_128_mnli',  \n",
    "            'cached_dev_bert-base-uncased_128_mnli',\n",
    "            'cached_dev_bert-base-uncased_128_mnli-mm'\n",
    "            ]:\n",
    "\n",
    "            features = torch.load(\n",
    "                os.path.join(\n",
    "                    self.glue_dir,feature_file))\n",
    "\n",
    "            self.features_array += features\n",
    "\n",
    "        self.all_input_ids = torch.tensor([f.input_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_input_masks = torch.tensor([f.input_mask for f in self.features_array], dtype=torch.long)\n",
    "        self.all_segment_ids = torch.tensor([f.segment_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_label_ids = torch.tensor([f.label_id for f in self.features_array], dtype=torch.long)\n",
    "\n",
    "        self.x_array = torch.stack((\n",
    "            self.all_input_ids,\n",
    "            self.all_input_masks,\n",
    "            self.all_segment_ids), dim=2)\n",
    "\n",
    "        assert np.all(np.array(self.all_label_ids) == self.y_array)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        a = self.confounder_array[idx]\n",
    "        x = self.x_array[idx, ...]\n",
    "        return x, y, a\n",
    "\n",
    "    def group_str(self, group_idx):\n",
    "        y = group_idx // (self.n_groups/self.n_classes)\n",
    "        c = group_idx % (self.n_groups//self.n_classes)\n",
    "\n",
    "        attr_name = self.confounder_names[0]\n",
    "        group_name = f'{self.target_name} = {int(y)}, {attr_name} = {int(c)}'\n",
    "        return group_name\n",
    "    \n",
    "data = MultiNLIDataset(root_dir = 'MultiNLI', target_name='gold_label_random',confounder_names=['sentence2_has_negation'])\n",
    "train_indices = [idx for idx, split in enumerate(data.split_array) if split == 0]\n",
    "training_set = torch.utils.data.Subset(data, train_indices)\n",
    "val_indices = [idx for idx, split in enumerate(data.split_array) if split == 1]\n",
    "vali_set = torch.utils.data.Subset(data, val_indices)\n",
    "test_indices = [idx for idx, split in enumerate(data.split_array) if split == 2]\n",
    "test_set = torch.utils.data.Subset(data, test_indices)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819a16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "training_data_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, drop_last=True)      \n",
    "valid_data_loader = torch.utils.data.DataLoader(vali_set, batch_size=batch_size, shuffle=True, drop_last=False)      \n",
    "test_data_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, drop_last=False)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0c95c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206170 82466 123713\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set),len(vali_set),len(test_set))\n",
    "k = iter(vali_set)\n",
    "x,y,a = training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a904016e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 128, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "# test, don't run\n",
    "x = iter(training_data_loader)\n",
    "y = next(x)\n",
    "a,b,c = y\n",
    "print(a.shape)\n",
    "\n",
    "from transformers import BertModel\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Assuming your tensor 'a' is of shape [batch_size, sequence_length, 3]\n",
    "input_ids = a[:,:,0] # Take all batch_size and sequence_length, only the 0th feature (token)\n",
    "attention_mask = a[:,:,1] # Take all batch_size and sequence_length, only the 1st feature (attention)\n",
    "token_type_ids = a[:,:,2] # Take all batch_size and sequence_length, only the 2nd feature (sequencebelong)\n",
    "\n",
    "# Now pass these separate tensors into the model\n",
    "out = bert_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "print(out.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865c6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def choose_value_patch(atten, value, p_dim):\n",
    "    # input insturction: \n",
    "    # atten: shape: Batch, Head, Patch\n",
    "    # value: Batch, Head, Patch, Dim\n",
    "    # Output: Batch, Head, Selct_Patch, dim\n",
    "    atten = atten[:,:,1:]\n",
    "    top_k_values, top_k_indices = torch.topk(atten, k=p_dim, dim=2, sorted=False)\n",
    "    #top_k_indices : Batch, Head, Select_patch\n",
    "    output = torch.gather(value, 2, top_k_indices.unsqueeze(-1).expand(-1,-1,-1,value.size(-1)))\n",
    "    return output\n",
    "\n",
    "class Last_Attention(nn.Module):\n",
    "    def __init__(self, type_id):\n",
    "        super(Last_Attention, self).__init__()\n",
    "        if type_id == 'base':\n",
    "            self.emb_size = 768\n",
    "        if type_id == 'large':\n",
    "            self.emb_size = 1024\n",
    "            \n",
    "        self.p_dim = 2\n",
    "        self.head = 8\n",
    "        self.temperature = 1#0.07\n",
    "        self.head_dim = self.emb_size //self.head\n",
    "        self.Q = nn.Linear(self.emb_size,self.emb_size)\n",
    "        self.K = nn.Linear(self.emb_size,self.emb_size)\n",
    "        self.V = nn.Linear(self.emb_size,self.emb_size)\n",
    "        self.projection = nn.Linear(self.emb_size, self.emb_size)\n",
    "        self.soft_max = nn.Softmax(dim=-1)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.p_dim*self.emb_size, self.emb_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_size, 128, bias=False),\n",
    "        )\n",
    "        self.cp = True\n",
    "        self.momentum = 0.1\n",
    "        if type_id == 'large':\n",
    "            self.register_buffer('running_mean_q', torch.zeros(1,self.head,128,128))\n",
    "            self.register_buffer('running_std_q', torch.ones(1,self.head,128,128))\n",
    "            self.register_buffer('running_mean_k', torch.zeros(1,self.head,128,128))\n",
    "            self.register_buffer('running_std_k', torch.ones(1,self.head,128,128))\n",
    "        if type_id == 'base':\n",
    "            self.register_buffer('running_mean_q', torch.zeros(1,self.head,128,96))\n",
    "            self.register_buffer('running_std_q', torch.ones(1,self.head,128,96))\n",
    "            self.register_buffer('running_mean_k', torch.zeros(1,self.head,128,96))\n",
    "            self.register_buffer('running_std_k', torch.ones(1,self.head,128,96))\n",
    "    #1, 8, 160, 96  qshape torch.Size([100, 8, 128, 96])\n",
    "    def register_buffer(self, name, tensor):\n",
    "        setattr(self, name, tensor)\n",
    "        \n",
    "    def forward(self, x, training):\n",
    "        B, N, C = x.shape\n",
    "        origin_k = self.K(x)\n",
    "        origin_q = self.Q(x)\n",
    "        origin_v = self.V(x)\n",
    "        \n",
    "        q = origin_q.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        k = origin_k.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "\n",
    "        \n",
    "        if training:\n",
    "            q_mean, q_std = torch.mean(q, 0, keepdim=True), torch.std(q, 0, keepdim=True)\n",
    "            k_mean, k_std = torch.mean(k, 0, keepdim=True), torch.std(k, 0, keepdim=True) \n",
    "\n",
    "            self.running_mean_q = (1 - self.momentum) * self.running_mean_q.to(device) + self.momentum * q_mean\n",
    "            self.running_std_q = (1 - self.momentum) * self.running_std_q.to(device) + self.momentum * q_std\n",
    "            self.running_mean_k = (1 - self.momentum) * self.running_mean_k.to(device) + self.momentum * k_mean\n",
    "            self.running_std_k = (1 - self.momentum) * self.running_std_k.to(device) + self.momentum * k_std\n",
    "        else:\n",
    "            q_mean = self.running_mean_q\n",
    "            q_std = self.running_std_q\n",
    "            k_mean = self.running_mean_k\n",
    "            k_std = self.running_std_k\n",
    "        \n",
    "        q = (q - q_mean) /q_std\n",
    "        k = (k - k_mean) /k_std\n",
    "        \n",
    "        v = origin_v.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        attention = (q @ k.transpose(-2,-1))* (self.head_dim ** (-0.5))        \n",
    "        atten = self.soft_max(attention/self.temperature)\n",
    "        out = (atten @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.projection(out)\n",
    "        \n",
    "        #attentions = atten.permute(0, 2, 1, 3) # Batch, Patch, Head, Patch\n",
    "        attentions = atten[:,:, 0, :]\n",
    "        \n",
    "        #fairness process\n",
    "        mst_val = choose_value_patch(attentions, v, self.p_dim)\n",
    "        mst_val = mst_val.reshape(B, -1)\n",
    "        mst_val = self.projector(mst_val)\n",
    "        z = F.normalize(mst_val)\n",
    "\n",
    "        return out, z\n",
    "\n",
    "    \n",
    "class Last_ATBlock(nn.Module):\n",
    "    def __init__(self, type_id):\n",
    "        super().__init__()\n",
    "        if type_id == 'base':\n",
    "            dim = 768\n",
    "        if type_id == 'large':\n",
    "            dim = 1024\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = Last_Attention(type_id)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)          \n",
    "        )\n",
    "        \n",
    "    def forward(self, x, training):\n",
    "        identity = x\n",
    "        x = self.norm(x)\n",
    "        x, vz = self.attention(x, training)\n",
    "        x += identity\n",
    "        res = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x += res\n",
    "        return x, vz\n",
    "\n",
    "    \n",
    "class BERT_base(nn.Module):\n",
    "    def __init__(self, BERT, type_id):\n",
    "        super(BERT_base, self).__init__()\n",
    "        if type_id == 'base':\n",
    "            p_dim = 768\n",
    "        if type_id == 'large':\n",
    "            p_dim = 1024\n",
    "        self.BERT = BERT\n",
    "        self.last_layer = Last_ATBlock(type_id)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(p_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, word, training):\n",
    "        input_ids = word[:,:,0] # (token)\n",
    "        attention_mask = word[:,:,1] # (attention)\n",
    "        token_type_ids = word[:,:,2] # (sequencebelong)\n",
    "        \n",
    "        x = self.BERT(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        x = x.last_hidden_state\n",
    "        hidden, v = self.last_layer(x, training)\n",
    "        y = self.seq(hidden[:,0])\n",
    "        return y, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cda7c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "epoch 0.000000 : 100%|██████████████████████████████████████████| 4123/4123 [38:31<00:00,  1.78batch/s, f=3.45, u=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28761  2209  3694]\n",
      " [ 1446 35901  3011]\n",
      " [ 3259  5511 31084]]\n",
      "acc: 0.8384001681310775\n",
      "DP: 0.471605778030412\n",
      "EoP 0.11784645348948697\n",
      "EoD 0.1052413542534535\n",
      "Trade off 0.7501657990306398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1.000000 : 100%|███████████████████████████████████████████| 4123/4123 [38:22<00:00,  1.79batch/s, f=3.89, u=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 34664     0]\n",
      " [    0 40358     0]\n",
      " [    0 39854     0]]\n",
      "acc: 0.333279445167444\n",
      "DP: 0.0\n",
      "EoP 0.0\n",
      "EoD 0.0\n",
      "Trade off 0.333279445167444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2.000000 : 100%|████████████████████████████████████████████| 4123/4123 [38:22<00:00,  1.79batch/s, f=3.89, u=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 34664     0]\n",
      " [    0 40358     0]\n",
      " [    0 39854     0]]\n",
      "acc: 0.333279445167444\n",
      "DP: 0.0\n",
      "EoP 0.0\n",
      "EoD 0.0\n",
      "Trade off 0.333279445167444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3.000000 : 100%|████████████████████████████████████████████| 4123/4123 [38:22<00:00,  1.79batch/s, f=3.89, u=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 34664     0]\n",
      " [    0 40358     0]\n",
      " [    0 39854     0]]\n",
      "acc: 0.333279445167444\n",
      "DP: 0.0\n",
      "EoP 0.0\n",
      "EoD 0.0\n",
      "Trade off 0.333279445167444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4.000000 : 100%|████████████████████████████████████████████| 4123/4123 [38:09<00:00,  1.80batch/s, f=3.89, u=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 34664     0]\n",
      " [    0 40358     0]\n",
      " [    0 39854     0]]\n",
      "acc: 0.333279445167444\n",
      "DP: 0.0\n",
      "EoP 0.0\n",
      "EoD 0.0\n",
      "Trade off 0.333279445167444\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel\n",
    "\n",
    "def compute_fairness(cf1, cf2):\n",
    "    dp = []\n",
    "    TPR = []\n",
    "    FPR = []\n",
    "    for cf in (cf1, cf2):\n",
    "        TP = np.diag(cf)\n",
    "        FN = cf.sum(axis =1)-np.diag(cf)\n",
    "        FP = cf.sum(axis = 0) - np.diag(cf)\n",
    "        TN = cf.sum()-(FN+FP+TP)\n",
    "\n",
    "        dp_value = (TP+FP)/(TN+FP+FN+TP)\n",
    "        TPR_value = TP/(TP+FN)\n",
    "        FPR_value = FP/(FP+TN)\n",
    "        dp.append(dp_value)\n",
    "        TPR.append(TPR_value)\n",
    "        FPR.append(FPR_value)\n",
    "    DP = abs(dp[0]-dp[1])\n",
    "    EoP = abs(TPR[0] - TPR[1])\n",
    "    EoD = 0.5*(abs(FPR[0]-FPR[1])+abs(TPR[0]-TPR[1]))\n",
    "    return DP, EoP, EoD  \n",
    "\n",
    "class SupervisedContrastiveLoss_v2(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Implementation of the loss described in the paper Supervised Contrastive Learning :\n",
    "        https://arxiv.org/abs/2004.11362\n",
    "        :param temperature: int\n",
    "        \"\"\"\n",
    "        super(SupervisedContrastiveLoss_v2, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, projections, targets):\n",
    "        dot_product_tempered = torch.mm(projections, projections.T) / self.temperature\n",
    "        exp_dot_tempered = (\n",
    "            torch.exp(dot_product_tempered - torch.max(dot_product_tempered, dim=1, keepdim=True)[0]) + 1e-5\n",
    "        )\n",
    "\n",
    "        mask_similar_class = (targets.unsqueeze(1).repeat(1, targets.shape[0]) == targets).to(device)\n",
    "        mask_anchor_out = (1 - torch.eye(exp_dot_tempered.shape[0])).to(device)\n",
    "        mask_combined = mask_similar_class * mask_anchor_out\n",
    "        cardinality_per_samples = torch.sum(mask_combined, dim=1)\n",
    "\n",
    "        log_prob = -torch.log(exp_dot_tempered / (torch.sum(exp_dot_tempered * mask_anchor_out, dim=1, keepdim=True)))\n",
    "        supervised_contrastive_loss_per_sample = torch.sum(log_prob * mask_combined, dim=1) / cardinality_per_samples\n",
    "        supervised_contrastive_loss = torch.mean(supervised_contrastive_loss_per_sample)\n",
    "\n",
    "        return supervised_contrastive_loss\n",
    "    \n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "#x,y,a\n",
    "def train_model(type_id):\n",
    "    epoch = 5\n",
    "    if type_id == 'base':\n",
    "        BERT = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        lr = 3e-5\n",
    "    if type_id =='large':\n",
    "        BERT = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "        lr = 1e-5\n",
    "        \n",
    "    model = BERT_base(BERT, type_id).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    acc = 0\n",
    "    fair_criterion = SupervisedContrastiveLoss_v2()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-5)#1e-4\n",
    "    \n",
    "    for epoches in range(epoch):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            for word, train_target, _ in tepoch:\n",
    "                word = word.to(device)\n",
    "                train_target = train_target.to(device)\n",
    "                train_target_onehot = torch.nn.functional.one_hot(train_target, num_classes=3)\n",
    "                train_target_onehot = train_target_onehot.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs, v = model(word, training=True)\n",
    "                ut_loss = criterion(outputs, train_target_onehot)\n",
    "                fair_loss = fair_criterion(v, train_target.squeeze())\n",
    "                loss =  ut_loss + 0.8*fair_loss\n",
    "                tepoch.set_postfix(u= ut_loss.item(), f= fair_loss.item()) \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "                \n",
    "             \n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "    # Evaluate on test set.\n",
    "        for step, (test_word, test_target, test_sensitive) in enumerate(test_data_loader):\n",
    "            test_word = test_word.to(device)\n",
    "            test_target = test_target.to(device)\n",
    "            \n",
    "            gt = test_target.detach().cpu().numpy()\n",
    "            sen = test_sensitive.detach().cpu().numpy()\n",
    "            test_gt.extend(gt)\n",
    "            sense_gt.extend(sen)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction,_ = model(test_word, training=False)\n",
    "                test_pred_ = torch.argmax(prediction, dim=1)\n",
    "                test_pred.extend(test_pred_.detach().cpu().numpy())\n",
    "\n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "        female_CM = confusion_matrix(female_gt, female_predic) \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        DP, EoP, EoD = compute_fairness(female_CM , male_CM)\n",
    "        ACC = accuracy_score(test_gt, test_pred)\n",
    "        print(female_CM)\n",
    "        print('acc:', ACC)\n",
    "        print('DP:', max(DP))\n",
    "        print('EoP' , max(EoP))\n",
    "        print('EoD', max(EoD))\n",
    "        print('Trade off', ACC *(1-max(EoD)))\n",
    "        \n",
    "        \n",
    "seed_everything(1024)    \n",
    "train_model('large')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
