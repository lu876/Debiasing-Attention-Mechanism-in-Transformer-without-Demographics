{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f822e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once is enough!!\n",
    "\"\"\"\n",
    "##################################################################################\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "################ Paths and other configs - Set these #################################\n",
    "\n",
    "data_dir = 'MultiNLI'\n",
    "glue_dir = 'MultiNLI'\n",
    "\n",
    "type_of_split = 'random'\n",
    "assert type_of_split in ['preset', 'random']\n",
    "# If 'preset', use the official train/val/test MultiNLI split\n",
    "# If 'random', randomly split 50%/20%/30% of the data to train/val/test\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "def tokenize(s):\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = s.lower()\n",
    "    s = s.split(' ')\n",
    "    return s\n",
    "\n",
    "### Read in data and assign train/val/test splits\n",
    "train_df = pd.read_json(\n",
    "    os.path.join(\n",
    "        data_dir,\n",
    "        'multinli_1.0_train.jsonl'),\n",
    "    lines=True)\n",
    "\n",
    "val_df = pd.read_json(\n",
    "    os.path.join(\n",
    "        data_dir,\n",
    "        'multinli_1.0_dev_matched.jsonl'),\n",
    "    lines=True)\n",
    "\n",
    "test_df = pd.read_json(\n",
    "    os.path.join(\n",
    "        data_dir,\n",
    "        'multinli_1.0_dev_mismatched.jsonl'),\n",
    "    lines=True)\n",
    "\n",
    "split_dict = {\n",
    "    'train': 0,\n",
    "    'val': 1,\n",
    "    'test': 2\n",
    "}\n",
    "\n",
    "if type_of_split == 'preset':\n",
    "    train_df['split'] = split_dict['train']\n",
    "    val_df['split'] = split_dict['val']\n",
    "    test_df['split'] = split_dict['test']\n",
    "    df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "elif type_of_split == 'random':\n",
    "    val_frac = 0.2\n",
    "    test_frac = 0.3\n",
    "\n",
    "    df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "    n = len(df)\n",
    "    n_val = int(val_frac * n)\n",
    "    n_test = int(test_frac * n)\n",
    "    n_train = n - n_val - n_test\n",
    "    splits = np.array([split_dict['train']] * n_train + [split_dict['val']] * n_val + [split_dict['test']] * n_test)\n",
    "    np.random.shuffle(splits)\n",
    "    df['split'] = splits\n",
    "\n",
    "### Assign labels\n",
    "df = df.loc[df['gold_label'] != '-', :]\n",
    "print(f'Total number of examples: {len(df)}')\n",
    "for k, v in split_dict.items():\n",
    "    print(k, np.mean(df['split'] == v))\n",
    "\n",
    "label_dict = {\n",
    "    'contradiction': 0,\n",
    "    'entailment': 1,\n",
    "    'neutral': 2\n",
    "}\n",
    "for k, v in label_dict.items():\n",
    "    idx = df.loc[:, 'gold_label'] == k\n",
    "    df.loc[idx, 'gold_label'] = v\n",
    "\n",
    "### Assign spurious attribute (negation words)\n",
    "negation_words = ['nobody', 'no', 'never', 'nothing'] # Taken from https://arxiv.org/pdf/1803.02324.pdf\n",
    "\n",
    "df['sentence2_has_negation'] = [False] * len(df)\n",
    "\n",
    "for negation_word in negation_words:\n",
    "    df['sentence2_has_negation'] |= [negation_word in tokenize(sentence) for sentence in df['sentence2']]\n",
    "\n",
    "df['sentence2_has_negation'] = df['sentence2_has_negation'].astype(int)\n",
    "\n",
    "## Write to disk\n",
    "df = df[['gold_label', 'sentence2_has_negation', 'split']]\n",
    "df.to_csv(os.path.join(data_dir, f'metadata_{type_of_split}.csv'))\n",
    "print('Done')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7d7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenyu/miniconda3/envs/DLcourse/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382945\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, Subset    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "class MultiNLIDataset():\n",
    "    \"\"\"\n",
    "    MultiNLI dataset.\n",
    "    label_dict = {\n",
    "        'contradiction': 0,\n",
    "        'entailment': 1,\n",
    "        'neutral': 2\n",
    "    }\n",
    "    # Negation words taken from https://arxiv.org/pdf/1803.02324.pdf\n",
    "    negation_words = ['nobody', 'no', 'never', 'nothing']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir,\n",
    "                 target_name, confounder_names,\n",
    "                 augment_data=False,\n",
    "                 model_type=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_name = target_name\n",
    "        self.confounder_names = confounder_names\n",
    "        self.model_type = model_type\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        assert len(confounder_names) == 1\n",
    "        assert confounder_names[0] == 'sentence2_has_negation'\n",
    "        assert target_name in ['gold_label_preset', 'gold_label_random']\n",
    "        assert augment_data == False\n",
    "\n",
    "        self.data_dir = os.path.join(\n",
    "            self.root_dir)\n",
    "        self.glue_dir = os.path.join(\n",
    "            self.root_dir)\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.data_dir} does not exist yet. Please generate the dataset first.')\n",
    "        if not os.path.exists(self.glue_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.glue_dir} does not exist yet. Please generate the dataset first.')\n",
    "\n",
    "        # Read in metadata\n",
    "        type_of_split = target_name.split('_')[-1]\n",
    "        self.metadata_df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                self.data_dir,\n",
    "                f'metadata_{type_of_split}.csv'),\n",
    "            index_col=0)\n",
    "\n",
    "        # Get the y values\n",
    "        # gold_label is hardcoded\n",
    "        self.y_array = self.metadata_df['gold_label'].values\n",
    "        self.n_classes = len(np.unique(self.y_array))\n",
    "\n",
    "        self.confounder_array = self.metadata_df[confounder_names[0]].values\n",
    "        print(np.sum(self.confounder_array==0))\n",
    "        self.n_confounders = len(confounder_names)\n",
    "\n",
    "        # Map to groups\n",
    "        self.n_groups = len(np.unique(self.confounder_array)) * self.n_classes\n",
    "        self.group_array = (self.y_array*(self.n_groups/self.n_classes) + self.confounder_array).astype('int')\n",
    "\n",
    "        # Extract splits\n",
    "        self.split_array = self.metadata_df['split'].values\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "\n",
    "        # Load features\n",
    "        self.features_array = []\n",
    "        for feature_file in [\n",
    "            'cached_train_bert-base-uncased_128_mnli',  \n",
    "            'cached_dev_bert-base-uncased_128_mnli',\n",
    "            'cached_dev_bert-base-uncased_128_mnli-mm'\n",
    "            ]:\n",
    "\n",
    "            features = torch.load(\n",
    "                os.path.join(\n",
    "                    self.glue_dir,feature_file))\n",
    "\n",
    "            self.features_array += features\n",
    "\n",
    "        self.all_input_ids = torch.tensor([f.input_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_input_masks = torch.tensor([f.input_mask for f in self.features_array], dtype=torch.long)\n",
    "        self.all_segment_ids = torch.tensor([f.segment_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_label_ids = torch.tensor([f.label_id for f in self.features_array], dtype=torch.long)\n",
    "\n",
    "        self.x_array = torch.stack((\n",
    "            self.all_input_ids,\n",
    "            self.all_input_masks,\n",
    "            self.all_segment_ids), dim=2)\n",
    "\n",
    "        assert np.all(np.array(self.all_label_ids) == self.y_array)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        a = self.confounder_array[idx]\n",
    "        x = self.x_array[idx, ...]\n",
    "        return x, y, a\n",
    "\n",
    "    def group_str(self, group_idx):\n",
    "        y = group_idx // (self.n_groups/self.n_classes)\n",
    "        c = group_idx % (self.n_groups//self.n_classes)\n",
    "\n",
    "        attr_name = self.confounder_names[0]\n",
    "        group_name = f'{self.target_name} = {int(y)}, {attr_name} = {int(c)}'\n",
    "        return group_name\n",
    "    \n",
    "data = MultiNLIDataset(root_dir = 'MultiNLI', target_name='gold_label_random',confounder_names=['sentence2_has_negation'])\n",
    "train_indices = [idx for idx, split in enumerate(data.split_array) if split == 0]\n",
    "training_set = torch.utils.data.Subset(data, train_indices)\n",
    "val_indices = [idx for idx, split in enumerate(data.split_array) if split == 1]\n",
    "vali_set = torch.utils.data.Subset(data, val_indices)\n",
    "test_indices = [idx for idx, split in enumerate(data.split_array) if split == 2]\n",
    "test_set = torch.utils.data.Subset(data, test_indices)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819a16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "training_data_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, drop_last=True)      \n",
    "valid_data_loader = torch.utils.data.DataLoader(vali_set, batch_size=batch_size, shuffle=True, drop_last=False)      \n",
    "test_data_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, drop_last=False)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0c95c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206170 82466 123713\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set),len(vali_set),len(test_set))\n",
    "k = iter(vali_set)\n",
    "x,y,a = training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a904016e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "# test, don't run\n",
    "x = iter(training_data_loader)\n",
    "y = next(x)\n",
    "a,b,c = y\n",
    "print(a.shape)\n",
    "\n",
    "from transformers import BertModel\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Assuming your tensor 'a' is of shape [batch_size, sequence_length, 3]\n",
    "input_ids = a[:,:,0] # Take all batch_size and sequence_length, only the 0th feature (token)\n",
    "attention_mask = a[:,:,1] # Take all batch_size and sequence_length, only the 1st feature (attention)\n",
    "token_type_ids = a[:,:,2] # Take all batch_size and sequence_length, only the 2nd feature (sequencebelong)\n",
    "\n",
    "# Now pass these separate tensors into the model\n",
    "out = bert_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "print(out.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb483377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoCo(nn.Module):\n",
    "    \"\"\"\n",
    "    Build a MoCo model with: a query encoder, a key encoder, and a queue\n",
    "    Reference https://arxiv.org/abs/1911.05722\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder, dim=128, K=1500, m=0.999, T=0.07, mlp=False):\n",
    "        super(MoCo, self).__init__()\n",
    "\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "\n",
    "        # create the encoders\n",
    "        self.encoder_q = base_encoder\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "        self.register_buffer(\"label_queue\", torch.zeros(K).long())\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "   \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys, labes):\n",
    "        # gather keys before updating queue\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        self.label_queue[ptr:ptr + batch_size] = labes\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "  \n",
    "\n",
    "    def forward(self, word, labels, training=True):\n",
    "        # compute query features\n",
    "        z, q = self.encoder_q(word, training=training)  # queries: NxC\n",
    "        \n",
    "        # dequeue and enqueue\n",
    "        self._dequeue_and_enqueue(q, labels)\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=1)\n",
    "        return self.queue, self.label_queue, z\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _inference(self,word, attention):\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            z, q = self.encoder_q(word, training=False)  # queries: NxC\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865c6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_value_patch(atten, value, p_dim):\n",
    "    # input insturction: \n",
    "    # atten: shape: Batch, Head, Patch\n",
    "    # value: Batch, Head, Patch, Dim\n",
    "    # Output: Batch, Head, Selct_Patch, dim\n",
    "    atten = atten[:,:,1:]\n",
    "    top_k_values, top_k_indices = torch.topk(atten, k=p_dim, dim=2, sorted=False)\n",
    "    #top_k_indices : Batch, Head, Select_patch\n",
    "    output = torch.gather(value, 2, top_k_indices.unsqueeze(-1).expand(-1,-1,-1,value.size(-1)))\n",
    "    return output\n",
    "\n",
    "class Last_Attention(nn.Module):\n",
    "    def __init__(self, type_id):\n",
    "        super(Last_Attention, self).__init__()\n",
    "        if type_id == 'base':\n",
    "            self.emb_size = 768\n",
    "        if type_id == 'large':\n",
    "            self.emb_size = 1024\n",
    "            \n",
    "        self.p_dim = 2\n",
    "        self.head = 8\n",
    "        self.temperature = 1\n",
    "        self.head_dim = self.emb_size //self.head\n",
    "        self.Q = nn.Linear(self.emb_size,self.emb_size)\n",
    "        self.K = nn.Linear(self.emb_size,self.emb_size)\n",
    "        self.V = nn.Linear(self.emb_size,self.emb_size)\n",
    "        self.projection = nn.Linear(self.emb_size, self.emb_size)\n",
    "        self.soft_max = nn.Softmax(dim=-1)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.p_dim*self.emb_size, self.emb_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_size, 128, bias=False),\n",
    "        )\n",
    "        self.cp = True\n",
    "        self.momentum = 0.1\n",
    "        if type_id == 'large':\n",
    "            self.register_buffer('running_mean_q', torch.zeros(1,self.head,128,128))\n",
    "            self.register_buffer('running_std_q', torch.ones(1,self.head,128,128))\n",
    "            self.register_buffer('running_mean_k', torch.zeros(1,self.head,128,128))\n",
    "            self.register_buffer('running_std_k', torch.ones(1,self.head,128,128))\n",
    "        if type_id == 'base':\n",
    "            self.register_buffer('running_mean_q', torch.zeros(1,self.head,128,96))\n",
    "            self.register_buffer('running_std_q', torch.ones(1,self.head,128,96))\n",
    "            self.register_buffer('running_mean_k', torch.zeros(1,self.head,128,96))\n",
    "            self.register_buffer('running_std_k', torch.ones(1,self.head,128,96))\n",
    "    #1, 8, 160, 96  qshape torch.Size([100, 8, 128, 96])\n",
    "    def register_buffer(self, name, tensor):\n",
    "        setattr(self, name, tensor)\n",
    "        \n",
    "    def forward(self, x, training):\n",
    "        B, N, C = x.shape\n",
    "        origin_k = self.K(x)\n",
    "        origin_q = self.Q(x)\n",
    "        origin_v = self.V(x)\n",
    "        \n",
    "        q = origin_q.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        k = origin_k.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        \n",
    "        self.running_mean_q = self.running_mean_q.detach()\n",
    "        self.running_std_q = self.running_std_q.detach()\n",
    "        self.running_mean_k = self.running_mean_k.detach()\n",
    "        self.running_std_k = self.running_std_k.detach()\n",
    "\n",
    "        \n",
    "        if training:\n",
    "            q_mean, q_std = torch.mean(q, 0, keepdim=True), torch.std(q, 0, keepdim=True)\n",
    "            k_mean, k_std = torch.mean(k, 0, keepdim=True), torch.std(k, 0, keepdim=True) \n",
    "\n",
    "            self.running_mean_q = (1 - self.momentum) * self.running_mean_q.to(device) + self.momentum * q_mean\n",
    "            self.running_std_q = (1 - self.momentum) * self.running_std_q.to(device) + self.momentum * q_std\n",
    "            self.running_mean_k = (1 - self.momentum) * self.running_mean_k.to(device) + self.momentum * k_mean\n",
    "            self.running_std_k = (1 - self.momentum) * self.running_std_k.to(device) + self.momentum * k_std\n",
    "        else:\n",
    "            q_mean = self.running_mean_q\n",
    "            q_std = self.running_std_q\n",
    "            k_mean = self.running_mean_k\n",
    "            k_std = self.running_std_k\n",
    "        \n",
    "        q = (q - q_mean) /q_std\n",
    "        k = (k - k_mean) /k_std\n",
    "        \n",
    "        v = origin_v.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        attention = (q @ k.transpose(-2,-1))* (self.head_dim ** (-0.5))        \n",
    "        atten = self.soft_max(attention/self.temperature)\n",
    "        out = (atten @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.projection(out)\n",
    "        \n",
    "        #attentions = atten.permute(0, 2, 1, 3) # Batch, Patch, Head, Patch\n",
    "        attentions = atten[:,:, 0, :]\n",
    "        \n",
    "        #fairness process\n",
    "        mst_val = choose_value_patch(attentions, v, self.p_dim)\n",
    "        mst_val = mst_val.reshape(B, -1)\n",
    "        mst_val = self.projector(mst_val)\n",
    "        z = F.normalize(mst_val)\n",
    "\n",
    "        return out, z\n",
    "\n",
    "    \n",
    "class Last_ATBlock(nn.Module):\n",
    "    def __init__(self, type_id):\n",
    "        super().__init__()\n",
    "        if type_id == 'base':\n",
    "            dim = 768\n",
    "        if type_id == 'large':\n",
    "            dim = 1024\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = Last_Attention(type_id)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)          \n",
    "        )\n",
    "        \n",
    "    def forward(self, x, training):\n",
    "        identity = x\n",
    "        x = self.norm(x)\n",
    "        x, vz = self.attention(x, training)\n",
    "        x += identity\n",
    "        res = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x += res\n",
    "        return x, vz\n",
    "\n",
    "    \n",
    "class BERT_base(nn.Module):\n",
    "    def __init__(self, BERT, type_id):\n",
    "        super(BERT_base, self).__init__()\n",
    "        if type_id == 'base':\n",
    "            p_dim = 768\n",
    "        if type_id == 'large':\n",
    "            p_dim = 1024\n",
    "        self.BERT = BERT\n",
    "        self.last_layer = Last_ATBlock(type_id)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(p_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, word, training):\n",
    "        input_ids = word[:,:,0] # (token)\n",
    "        attention_mask = word[:,:,1] # (attention)\n",
    "        token_type_ids = word[:,:,2] # (sequencebelong)\n",
    "        \n",
    "        x = self.BERT(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        x = x.last_hidden_state\n",
    "        hidden, v = self.last_layer(x, training)\n",
    "        y = self.seq(hidden[:,0])\n",
    "        return y, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cda7c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "epoch 0.000000 : 100%|██████████████████████████████████████████| 2061/2061 [12:00<00:00,  2.86batch/s, f=6.93, u=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27272  2736  4656]\n",
      " [ 1481 34806  4071]\n",
      " [ 3459  4577 31818]]\n",
      "acc: 0.8230016247282016\n",
      "DP: 0.5140936966644006\n",
      "EoP 0.23118715795647304\n",
      "EoD 0.17715169391144586\n",
      "Trade off 0.6772054928157285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1.000000 : 100%|██████████████████████████████████████████| 2061/2061 [11:56<00:00,  2.88batch/s, f=6.89, u=0.416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28791  1988  3885]\n",
      " [ 1995 34233  4130]\n",
      " [ 4286  4433 31135]]\n",
      "acc: 0.82525684447067\n",
      "DP: 0.4822944915560694\n",
      "EoP 0.18258702085742207\n",
      "EoD 0.134722739557246\n",
      "Trade off 0.7140759815452133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2.000000 : 100%|██████████████████████████████████████████| 2061/2061 [11:56<00:00,  2.87batch/s, f=6.86, u=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28636  1765  4263]\n",
      " [ 2025 32947  5386]\n",
      " [ 4209  3773 31872]]\n",
      "acc: 0.8194611722292726\n",
      "DP: 0.4879003688733704\n",
      "EoP 0.20363053888188487\n",
      "EoD 0.14726130509209204\n",
      "Trade off 0.6987862505344944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3.000000 : 100%|██████████████████████████████████████████| 2061/2061 [11:54<00:00,  2.88batch/s, f=6.83, u=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28837  1695  4132]\n",
      " [ 2381 32812  5165]\n",
      " [ 4502  3852 31500]]\n",
      "acc: 0.8170362047642528\n",
      "DP: 0.4733719699526305\n",
      "EoP 0.15688150354235142\n",
      "EoD 0.12527439340863622\n",
      "Trade off 0.7146824898195167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4.000000 : 100%|██████████████████████████████████████████| 2061/2061 [11:55<00:00,  2.88batch/s, f=6.83, u=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29562  1659  3443]\n",
      " [ 2829 32876  4653]\n",
      " [ 5580  4281 29993]]\n",
      "acc: 0.810787871929385\n",
      "DP: 0.4475530967254819\n",
      "EoP 0.12587120711686095\n",
      "EoD 0.09972004017233231\n",
      "Trade off 0.7299360727693469\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel\n",
    "\n",
    "def compute_fairness(cf1, cf2):\n",
    "    dp = []\n",
    "    TPR = []\n",
    "    FPR = []\n",
    "    for cf in (cf1, cf2):\n",
    "        TP = np.diag(cf)\n",
    "        FN = cf.sum(axis =1)-np.diag(cf)\n",
    "        FP = cf.sum(axis = 0) - np.diag(cf)\n",
    "        TN = cf.sum()-(FN+FP+TP)\n",
    "\n",
    "        dp_value = (TP+FP)/(TN+FP+FN+TP)\n",
    "        TPR_value = TP/(TP+FN)\n",
    "        FPR_value = FP/(FP+TN)\n",
    "        dp.append(dp_value)\n",
    "        TPR.append(TPR_value)\n",
    "        FPR.append(FPR_value)\n",
    "    DP = abs(dp[0]-dp[1])\n",
    "    EoP = abs(TPR[0] - TPR[1])\n",
    "    EoD = 0.5*(abs(FPR[0]-FPR[1])+abs(TPR[0]-TPR[1]))\n",
    "    return DP, EoP, EoD  \n",
    "\n",
    "class SupervisedContrastiveLoss_v2(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Implementation of the loss described in the paper Supervised Contrastive Learning :\n",
    "        https://arxiv.org/abs/2004.11362\n",
    "        :param temperature: int\n",
    "        \"\"\"\n",
    "        super(SupervisedContrastiveLoss_v2, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, projections, targets):\n",
    "        dot_product_tempered = torch.mm(projections, projections.T) / self.temperature\n",
    "        exp_dot_tempered = (\n",
    "            torch.exp(dot_product_tempered - torch.max(dot_product_tempered, dim=1, keepdim=True)[0]) + 1e-5\n",
    "        )\n",
    "\n",
    "        mask_similar_class = (targets.unsqueeze(1).repeat(1, targets.shape[0]) == targets).to(device)\n",
    "        mask_anchor_out = (1 - torch.eye(exp_dot_tempered.shape[0])).to(device)\n",
    "        mask_combined = mask_similar_class * mask_anchor_out\n",
    "        cardinality_per_samples = torch.sum(mask_combined, dim=1)\n",
    "\n",
    "        log_prob = -torch.log(exp_dot_tempered / (torch.sum(exp_dot_tempered * mask_anchor_out, dim=1, keepdim=True)))\n",
    "        supervised_contrastive_loss_per_sample = torch.sum(log_prob * mask_combined, dim=1) / cardinality_per_samples\n",
    "        supervised_contrastive_loss = torch.mean(supervised_contrastive_loss_per_sample)\n",
    "\n",
    "        return supervised_contrastive_loss\n",
    "    \n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "#x,y,a\n",
    "def train_model(type_id):\n",
    "    epoch = 5\n",
    "    if type_id == 'base':\n",
    "        BERT = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        lr = 5e-5\n",
    "    if type_id =='large':\n",
    "        BERT = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "        lr = 0.5e-5\n",
    "        \n",
    "    model = BERT_base(BERT, type_id).to(device)\n",
    "    model_moco = MoCo(model).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    acc = 0\n",
    "    fair_criterion = SupervisedContrastiveLoss_v2()\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model_moco.parameters()), lr=lr)\n",
    "    \n",
    "    for epoches in range(epoch):\n",
    "        \n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            model_moco.train()\n",
    "            for word, train_target, _ in tepoch:\n",
    "                word = word.to(device)\n",
    "                train_target = train_target.to(device)\n",
    "                train_target_onehot = torch.nn.functional.one_hot(train_target, num_classes=3)\n",
    "                train_target_onehot = train_target_onehot.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                features, targets, outputs = model_moco(word, train_target, training=True)\n",
    "                fair_loss = fair_criterion(features.T, targets)\n",
    "                ut_loss = criterion(outputs, train_target_onehot)\n",
    "                loss =  ut_loss + 0.8*fair_loss\n",
    "                tepoch.set_postfix(u= ut_loss.item(), f= fair_loss.item()) \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)\n",
    "                \n",
    "                \n",
    "          \n",
    "        model.eval()\n",
    "        model_moco.eval()\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "    # Evaluate on test set.\n",
    "        for step, (test_word, test_target, test_sensitive) in enumerate(test_data_loader):\n",
    "            test_word = test_word.to(device)\n",
    "            test_target = test_target.to(device)\n",
    "            \n",
    "            gt = test_target.detach().cpu().numpy()\n",
    "            sen = test_sensitive.detach().cpu().numpy()\n",
    "            test_gt.extend(gt)\n",
    "            sense_gt.extend(sen)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction,_ = model(test_word, training=False)\n",
    "                test_pred_ = torch.argmax(prediction, dim=1)\n",
    "                test_pred.extend(test_pred_.detach().cpu().numpy())\n",
    "\n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "        female_CM = confusion_matrix(female_gt, female_predic) \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        DP, EoP, EoD = compute_fairness(female_CM , male_CM)\n",
    "        ACC = accuracy_score(test_gt, test_pred)\n",
    "        print(female_CM)\n",
    "        print('acc:', ACC)\n",
    "        print('DP:', max(DP))\n",
    "        print('EoP' , max(EoP))\n",
    "        print('EoD', max(EoD))\n",
    "        print('Trade off', ACC *(1-max(EoD)))\n",
    "        \n",
    "        \n",
    "seed_everything(1024)    \n",
    "train_model('base')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
