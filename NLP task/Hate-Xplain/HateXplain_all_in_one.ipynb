{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel\n",
    "\n",
    "device = torch.device('cuda:2')\n",
    "training_set = pd.read_pickle('training_set.pkl')\n",
    "test_set = pd.read_pickle('test_set.pkl')\n",
    "\n",
    "def model_selection(m, moco, freeze):\n",
    "    if m =='bert_large':\n",
    "        bert_model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=False) \n",
    "        dim = 1024\n",
    "        if moco and freeze:\n",
    "            batch_size = 512\n",
    "            epoch = 100\n",
    "            lr = 1e-4\n",
    "        elif moco == False and freeze == True:\n",
    "            batch_size = 1024\n",
    "            epoch = 100\n",
    "            lr = 1e-4\n",
    "        elif moco == True and freeze == False:\n",
    "            batch_size = 16\n",
    "            epoch = 5\n",
    "            lr = 1e-5\n",
    "        elif moco == False and freeze == False:\n",
    "            batch_size = 25\n",
    "            epoch = 5\n",
    "            lr = 1e-5\n",
    "        \n",
    "    if m == 'bert':\n",
    "        bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False) \n",
    "        dim = 768\n",
    "        if moco and freeze:\n",
    "            batch_size = 128\n",
    "            epoch = 50\n",
    "            lr = 1e-4\n",
    "        elif moco == False and freeze == True:\n",
    "            batch_size = 1024\n",
    "            epoch = 50\n",
    "            lr = 1e-4\n",
    "        elif moco == True and freeze == False:\n",
    "            batch_size = 128\n",
    "            epoch = 5\n",
    "            lr = 1e-4\n",
    "        elif moco == False and freeze == False:\n",
    "            batch_size = 100\n",
    "            epoch = 5\n",
    "            lr = 1e-4\n",
    "    return bert_model, tokenizer, dim, batch_size, epoch, lr\n",
    "\n",
    "\n",
    "freeze = False\n",
    "moco = False\n",
    "m = 'bert' #model selection, {bert, bert_large}\n",
    "bert, tokenizer, dimension, batch_size, epoch, lr = model_selection(m, moco, freeze)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdf9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode_plus(training_set.text[128],padding = True, truncation=True, max_length = 160))\n",
    "Training_dataset_tokened = training_set['text'].map(lambda x: tokenizer(x, truncation=True,pad_to_max_length =True, max_length = 160, return_tensors='pt'))\n",
    "print(Training_dataset_tokened[0])\n",
    "Test_dataset_tokened = test_set['text'].map(lambda x: tokenizer(x, truncation=True,pad_to_max_length =True, max_length = 160, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_Hate_dataset(Dataset):\n",
    "    def __init__(self, training_corps, training_label): \n",
    "        # training_corps:Tokenized, training_label:training_set.final_label\n",
    "        self.train_token_word = training_corps\n",
    "        self.training_target = training_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.training_target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.train_token_word[idx]['input_ids']\n",
    "        attention = self.train_token_word[idx]['attention_mask']\n",
    "        label = torch.tensor(self.training_target[idx])\n",
    "        return word, attention, label\n",
    "    \n",
    "\n",
    "    \n",
    "class Test_Hate_dataset(Dataset):\n",
    "    def __init__(self, test_corps, test_label, test_sensitive): \n",
    "        # training_corps:Tokenized, training_label:training_set.final_label\n",
    "        self.test_token_word = test_corps\n",
    "        self.test_target = test_label\n",
    "        self.test_sensitive = test_sensitive\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.test_target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.test_token_word[idx]['input_ids']\n",
    "        attention = self.test_token_word[idx]['attention_mask']\n",
    "        label = torch.tensor(self.test_target[idx])\n",
    "        sensitive = torch.tensor(self.test_sensitive[idx])\n",
    "        return word, attention, label, sensitive\n",
    "\n",
    "\n",
    "THD_training_set = Training_Hate_dataset(Training_dataset_tokened, training_set.final_label)\n",
    "THD_test_set = Test_Hate_dataset(Test_dataset_tokened, test_set.final_label, test_set.final_target_category)\n",
    "training_data_loader = torch.utils.data.DataLoader(THD_training_set, batch_size=batch_size, shuffle=True, drop_last=True)      \n",
    "test_data_loader = torch.utils.data.DataLoader(THD_test_set, batch_size=batch_size, shuffle=False, drop_last=False)      \n",
    "itt = iter(THD_training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfecef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoCo(nn.Module):\n",
    "    \"\"\"\n",
    "    Build a MoCo model with: a query encoder, a key encoder, and a queue\n",
    "    reference https://arxiv.org/abs/1911.05722\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder, dim=128, K=128*14, m=0.999, T=0.07, mlp=False):\n",
    "        super(MoCo, self).__init__()\n",
    "\n",
    "        self.K = 512*3#K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "        # create the encoders\n",
    "        # num_classes is the output fc dimension\n",
    "        self.encoder_q = base_encoder\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "        self.register_buffer(\"label_queue\", torch.zeros(K).long())\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "   \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys, labes):\n",
    "        # gather keys before updating queue\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        self.label_queue[ptr:ptr + batch_size] = labes\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "  \n",
    "\n",
    "    def forward(self, word, attention, labels, training=True):\n",
    "        # compute query features\n",
    "        z, q = self.encoder_q(word, attention, training=training)  # queries: NxC\n",
    "        \n",
    "        # dequeue and enqueue\n",
    "        self._dequeue_and_enqueue(q, labels)\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=1)\n",
    "\n",
    "        return self.queue, self.label_queue, z\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _inference(self,word, attention):\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            z, q = self.encoder_q(word, attention, training=False)  # queries: NxC\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def choose_value_patch(atten, value, p_dim):\n",
    "    # input insturction: \n",
    "    # atten: shape: Batch, Head, Patch\n",
    "    # value: Batch, Head, Patch, Dim\n",
    "    # Output: Batch, Head, Selct_Patch, dim\n",
    "    atten = atten[:,:,1:]\n",
    "    top_k_values, top_k_indices = torch.topk(atten, k=p_dim, dim=2, sorted=False)\n",
    "    #top_k_indices : Batch, Head, Select_patch\n",
    "    output = torch.gather(value, 2, top_k_indices.unsqueeze(-1).expand(-1,-1,-1,value.size(-1)))\n",
    "    return output\n",
    "    \n",
    "class Last_Attention(nn.Module):\n",
    "    def __init__(self, dim, model):\n",
    "        super(Last_Attention, self).__init__()\n",
    "        self.p_dim = 2\n",
    "        self.emb_size = dim\n",
    "        self.head = 8\n",
    "        self.temperature = 0.07\n",
    "        self.head_dim = self.emb_size //self.head\n",
    "        self.Q = nn.Linear(dim,dim)\n",
    "        self.K = nn.Linear(dim,dim)\n",
    "        self.V = nn.Linear(dim,dim)\n",
    "        self.projection = nn.Linear(dim, dim)\n",
    "        self.soft_max = nn.Softmax(dim=-1)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.p_dim*dim, dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 128, bias=False),\n",
    "        )\n",
    "        self.cp = True\n",
    "        self.momentum = 0.1\n",
    "        if model == 'bert':\n",
    "            self.register_buffer('running_mean_q', torch.zeros(1,8,160,96))\n",
    "            self.register_buffer('running_std_q', torch.ones(1,8,160,96))\n",
    "            self.register_buffer('running_mean_k', torch.zeros(1,8,160,96))\n",
    "            self.register_buffer('running_std_k', torch.ones(1,8,160,96))\n",
    "        if model == 'bert_large':\n",
    "            self.register_buffer('running_mean_q', torch.zeros(1,8,160,128))\n",
    "            self.register_buffer('running_std_q', torch.ones(1,8,160,128))\n",
    "            self.register_buffer('running_mean_k', torch.zeros(1,8,160,128))\n",
    "            self.register_buffer('running_std_k', torch.ones(1,8,160,128))\n",
    "    #1, 8, 160, 96\n",
    "    def register_buffer(self, name, tensor):\n",
    "        setattr(self, name, tensor)\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        B, N, C = x.shape\n",
    "        origin_k = self.K(x)\n",
    "        origin_q = self.Q(x)       \n",
    "        origin_v = self.V(x)\n",
    "        self.running_mean_q = self.running_mean_q.detach()\n",
    "        self.running_std_q = self.running_std_q.detach()\n",
    "        self.running_mean_k = self.running_mean_k.detach()\n",
    "        self.running_std_k = self.running_std_k.detach()\n",
    "\n",
    "        \n",
    "        q = origin_q.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        k = origin_k.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        \n",
    "        if training:\n",
    "            q_mean, q_std = torch.mean(q, 0, keepdim=True), torch.std(q, 0, keepdim=True)\n",
    "            k_mean, k_std = torch.mean(k, 0, keepdim=True), torch.std(k, 0, keepdim=True) \n",
    "\n",
    "            self.running_mean_q = (1 - self.momentum) * self.running_mean_q.to(device) + self.momentum * q_mean\n",
    "            self.running_std_q = (1 - self.momentum) * self.running_std_q.to(device) + self.momentum * q_std\n",
    "            self.running_mean_k = (1 - self.momentum) * self.running_mean_k.to(device) + self.momentum * k_mean\n",
    "            self.running_std_k = (1 - self.momentum) * self.running_std_k.to(device) + self.momentum * k_std\n",
    "        else:\n",
    "            q_mean = self.running_mean_q\n",
    "            q_std = self.running_std_q\n",
    "            k_mean = self.running_mean_k\n",
    "            k_std = self.running_std_k\n",
    "        \n",
    "        q = (q - q_mean) /q_std\n",
    "        k = (k - k_mean) /k_std\n",
    "        \n",
    "        v = origin_v.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        attention = (q @ k.transpose(-2,-1))* (self.head_dim ** (-0.5))\n",
    "        atten = self.soft_max(attention/self.temperature)\n",
    "        out = (atten @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.projection(out)\n",
    "        attentions = atten[:,:, 0, :]\n",
    "\n",
    "        #fairness process\n",
    "        mst_val = choose_value_patch(attentions, v, self.p_dim)\n",
    "        mst_val = mst_val.reshape(B, -1)\n",
    "        mst_val = self.projector(mst_val)\n",
    "        return out, mst_val\n",
    "\n",
    "    \n",
    "class Last_ATBlock(nn.Module):\n",
    "    def __init__(self, dim, model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = Last_Attention(dim, model)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)          \n",
    "        )\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        identity = x\n",
    "        x = self.norm(x)\n",
    "        x, vz = self.attention(x, training)\n",
    "        x += identity\n",
    "        res = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x += res\n",
    "        return x, vz\n",
    "    \n",
    "    \n",
    "class BERT_model(nn.Module):\n",
    "    def __init__(self, BERT, dim, model):\n",
    "        super(BERT_model, self).__init__()\n",
    "        self.BERT = BERT\n",
    "        self.last_layer = Last_ATBlock(dim, model)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 1),     \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, word, attention, training=True):\n",
    "        x, _= self.BERT(word, attention, return_dict= False)\n",
    "        hidden, v = self.last_layer(x, training)\n",
    "        y = self.seq(hidden[:,0])\n",
    "        return y, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class SupLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07, base_temperature=None, K=128):\n",
    "        super(SupLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.base_temperature = temperature if base_temperature is None else base_temperature\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        ss = features.shape[0]\n",
    "        batch_size = (features.shape[0] - self.K) \n",
    "\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels[:batch_size], labels.T).float().to(device)\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features[:batch_size], features.T),\n",
    "            self.temperature)\n",
    "\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class SupervisedContrastiveLoss_v2(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Implementation of the loss described in the paper Supervised Contrastive Learning :\n",
    "        https://arxiv.org/abs/2004.11362\n",
    "        :param temperature: int\n",
    "        \"\"\"\n",
    "        super(SupervisedContrastiveLoss_v2, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, projections, targets):\n",
    "        dot_product_tempered = torch.mm(projections, projections.T) / self.temperature\n",
    "        exp_dot_tempered = (\n",
    "            torch.exp(dot_product_tempered - torch.max(dot_product_tempered, dim=1, keepdim=True)[0]) + 1e-5\n",
    "        )\n",
    "\n",
    "        mask_similar_class = (targets.unsqueeze(1).repeat(1, targets.shape[0]) == targets).to(device)\n",
    "        mask_anchor_out = (1 - torch.eye(exp_dot_tempered.shape[0])).to(device)\n",
    "        mask_combined = mask_similar_class * mask_anchor_out\n",
    "        cardinality_per_samples = torch.sum(mask_combined, dim=1)\n",
    "\n",
    "        log_prob = -torch.log(exp_dot_tempered / (torch.sum(exp_dot_tempered * mask_anchor_out, dim=1, keepdim=True)))\n",
    "        supervised_contrastive_loss_per_sample = torch.sum(log_prob * mask_combined, dim=1) / cardinality_per_samples\n",
    "        supervised_contrastive_loss = torch.mean(supervised_contrastive_loss_per_sample)\n",
    "\n",
    "        return supervised_contrastive_loss  \n",
    "    \n",
    "\n",
    "def train_model(moco, BERT, freeze, lr, epoch):\n",
    "    model = BERT_model(BERT, dimension, m).to(device)\n",
    "    if moco:\n",
    "        model_moco = MoCo(model).to(device)\n",
    "        fair_criterion = SupLoss()\n",
    "    else:\n",
    "        fair_criterion = SupervisedContrastiveLoss_v2()\n",
    "        \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    if freeze:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'BERT' in name:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    if moco:\n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model_moco.parameters()), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        \n",
    "    for epoches in range(epoch):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            for word, attention, train_target in tepoch:\n",
    "                # Transfer data to GPU if possible. \n",
    "                word = word.to(device) \n",
    "                attention = attention.to(device)\n",
    "                word = word.squeeze(1)\n",
    "                attention = attention.squeeze(1)\n",
    "                train_target = train_target.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                if moco:\n",
    "                    features, targets, outputs = model_moco(word, attention, train_target, training=True)\n",
    "                    fair_loss = fair_criterion(features.T, targets)\n",
    "                    train_target = train_target.unsqueeze(1)\n",
    "                    ut_loss = criterion(outputs, train_target)\n",
    "                    loss =  ut_loss + 0.1* fair_loss\n",
    "                    tepoch.set_postfix(ul = ut_loss.item(),fl = fair_loss.item())\n",
    "                else:\n",
    "                    outputs, v = model(word, attention)\n",
    "                    train_target = train_target.unsqueeze(1)\n",
    "                    ut_loss = criterion(outputs, train_target)\n",
    "                    fair_loss = fair_criterion(v, train_target.squeeze())\n",
    "                    loss =  ut_loss + 0.8 * fair_loss\n",
    "                    tepoch.set_postfix(u= ut_loss.item(), f= fair_loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)       \n",
    "                \n",
    "        if moco:\n",
    "            model_moco.eval()\n",
    "        else:\n",
    "            model.eval()\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "\n",
    "\n",
    "    # Evaluate\n",
    "        for step, (test_word, test_attention, test_target, test_sensitive) in enumerate(test_data_loader):\n",
    "            test_word = test_word.to(device)\n",
    "            test_attention = test_attention.to(device)\n",
    "            test_word = test_word.squeeze(1)\n",
    "            test_attention = test_attention.squeeze(1)\n",
    "            gt = test_target.detach().cpu().numpy()\n",
    "            sen = test_sensitive.detach().cpu().numpy()\n",
    "            test_gt.extend(gt)\n",
    "            sense_gt.extend(sen)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if moco:\n",
    "                    test_pred_ = model_moco._inference(test_word,test_attention)\n",
    "                else:\n",
    "                    test_pred_, _ = model(test_word,test_attention, training=False)\n",
    "                test_pred.extend(torch.round(test_pred_.squeeze(1)).detach().cpu().numpy())\n",
    "\n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "        female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "        male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "        female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "        male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0])\n",
    "        female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "        male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "\n",
    "        print('Female TPR', female_TPR)\n",
    "        print('male TPR', male_TPR)\n",
    "        print('DP',abs(female_dp - male_dp))\n",
    "        print('EOP', abs(female_TPR - male_TPR))\n",
    "        print('EoD',0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR)))\n",
    "        print('acc', accuracy_score(test_gt, test_pred))\n",
    "        print('Trade off',accuracy_score(test_gt, test_pred)*(1-0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR))) )\n",
    "\n",
    "        \n",
    "        \n",
    "seed_everything(4096)\n",
    "train_model(moco, bert, freeze, lr, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe2c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
