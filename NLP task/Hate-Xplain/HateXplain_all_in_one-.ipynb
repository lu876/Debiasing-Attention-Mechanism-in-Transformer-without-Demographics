{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc83fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenyu/miniconda3/envs/DLcourse/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel\n",
    "\n",
    "device = torch.device('cuda:2')\n",
    "device_id = 2\n",
    "training_set = pd.read_pickle('training_set.pkl')\n",
    "test_set = pd.read_pickle('test_set.pkl')\n",
    "\n",
    "def model_selection(m, moco, freeze):\n",
    "    if m =='bert_large':\n",
    "        bert_model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=False) \n",
    "        dim = 1024\n",
    "        if moco and freeze:\n",
    "            batch_size = 512\n",
    "            epoch = 100\n",
    "            lr = 1e-4\n",
    "        elif moco == False and freeze == True:\n",
    "            batch_size = 1024\n",
    "            epoch = 100\n",
    "            lr = 1e-4\n",
    "        elif moco == True and freeze == False:\n",
    "            batch_size = 16\n",
    "            epoch = 6\n",
    "            lr = 1e-5\n",
    "        elif moco == False and freeze == False:\n",
    "            batch_size = 25\n",
    "            epoch = 6\n",
    "            lr = 1e-5\n",
    "        \n",
    "    if m == 'bert':\n",
    "        bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False) \n",
    "        dim = 768\n",
    "        if moco and freeze:\n",
    "            batch_size = 128\n",
    "            epoch = 50\n",
    "            lr = 1e-4\n",
    "        elif moco == False and freeze == True:\n",
    "            batch_size = 1024\n",
    "            epoch = 50\n",
    "            lr =3e-4\n",
    "        elif moco == True and freeze == False:\n",
    "            batch_size = 128\n",
    "            epoch =6\n",
    "            lr = 1e-4\n",
    "        elif moco == False and freeze == False:\n",
    "            batch_size = 128\n",
    "            epoch =6\n",
    "            lr = 2e-5\n",
    "    return bert_model, tokenizer, dim, batch_size, epoch, lr\n",
    "\n",
    "\n",
    "freeze = False\n",
    "moco = False\n",
    "m = 'bert_large' #model selection, {bert, bert_large}\n",
    "bert, tokenizer, dimension, batch_size, epoch, lr = model_selection(m, moco, freeze)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcdf9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 15504, 1996, 5181, 2138, 2009, 18897, 4913, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenyu/miniconda3/envs/DLcourse/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2057,  3685,  3613,  4214,  9731, 10469,  2015,  2065,  1996,\n",
      "          2916,  1997,  2035, 24185, 22984,  2078,  4995,  2102,  8280,  2748,\n",
      "          2000,  1037,  4424, 18421,  2270,  2862,  2021,  2097,  1037,  9099,\n",
      "         11690, 22437,  1998, 19483, 24185, 22984,  2078,  2022,  2583,  2000,\n",
      "          4607,  2037,  2592,  2006,  1996,  7316,  7123,  5907,  7057,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode_plus(training_set.text[128],padding = True, truncation=True, max_length = 160))\n",
    "Training_dataset_tokened = training_set['text'].map(lambda x: tokenizer(x, truncation=True,pad_to_max_length =True, max_length = 160, return_tensors='pt'))\n",
    "print(Training_dataset_tokened[0])\n",
    "Test_dataset_tokened = test_set['text'].map(lambda x: tokenizer(x, truncation=True,pad_to_max_length =True, max_length = 160, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe5f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_Hate_dataset(Dataset):\n",
    "    def __init__(self, training_corps, training_label): \n",
    "        # training_corps:Tokenized, training_label:training_set.final_label\n",
    "        self.train_token_word = training_corps\n",
    "        self.training_target = training_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.training_target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.train_token_word[idx]['input_ids']\n",
    "        attention = self.train_token_word[idx]['attention_mask']\n",
    "        label = torch.tensor(self.training_target[idx])\n",
    "        return word, attention, label\n",
    "    \n",
    "\n",
    "    \n",
    "class Test_Hate_dataset(Dataset):\n",
    "    def __init__(self, test_corps, test_label, test_sensitive): \n",
    "        # training_corps:Tokenized, training_label:training_set.final_label\n",
    "        self.test_token_word = test_corps\n",
    "        self.test_target = test_label\n",
    "        self.test_sensitive = test_sensitive\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.test_target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.test_token_word[idx]['input_ids']\n",
    "        attention = self.test_token_word[idx]['attention_mask']\n",
    "        label = torch.tensor(self.test_target[idx])\n",
    "        sensitive = torch.tensor(self.test_sensitive[idx])\n",
    "        return word, attention, label, sensitive\n",
    "\n",
    "\n",
    "THD_training_set = Training_Hate_dataset(Training_dataset_tokened, training_set.final_label)\n",
    "THD_test_set = Test_Hate_dataset(Test_dataset_tokened, test_set.final_label, test_set.final_target_category)\n",
    "training_data_loader = torch.utils.data.DataLoader(THD_training_set, batch_size=batch_size, shuffle=True, drop_last=True)      \n",
    "test_data_loader = torch.utils.data.DataLoader(THD_test_set, batch_size=batch_size, shuffle=False, drop_last=False)      \n",
    "itt = iter(THD_training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cfecef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoCo(nn.Module):\n",
    "    \"\"\"\n",
    "    Build a MoCo model with: a query encoder, a key encoder, and a queue\n",
    "    reference https://arxiv.org/abs/1911.05722\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder, dim=128, K=128*14, m=0.999, T=0.07, mlp=False):\n",
    "        super(MoCo, self).__init__()\n",
    "\n",
    "        self.K = 512*3#K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "        # create the encoders\n",
    "        # num_classes is the output fc dimension\n",
    "        self.encoder_q = base_encoder\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "        self.register_buffer(\"label_queue\", torch.zeros(K).long())\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "   \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys, labes):\n",
    "        # gather keys before updating queue\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        self.label_queue[ptr:ptr + batch_size] = labes\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "  \n",
    "\n",
    "    def forward(self, word, attention, labels, training=True):\n",
    "        # compute query features\n",
    "        z, q = self.encoder_q(word, attention, training=training)  # queries: NxC\n",
    "        \n",
    "        # dequeue and enqueue\n",
    "        self._dequeue_and_enqueue(q, labels)\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=1)\n",
    "\n",
    "        return self.queue, self.label_queue, z\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _inference(self,word, attention):\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            z, q = self.encoder_q(word, attention, training=False)  # queries: NxC\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5f8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def choose_value_patch(atten, value, p_dim):\n",
    "    # input insturction: \n",
    "    # atten: shape: Batch, Head, Patch\n",
    "    # value: Batch, Head, Patch, Dim\n",
    "    # Output: Batch, Head, Selct_Patch, dim\n",
    "    atten = atten[:,:,1:]\n",
    "    top_k_values, top_k_indices = torch.topk(atten, k=p_dim, dim=2, sorted=False)\n",
    "    #top_k_indices : Batch, Head, Select_patch\n",
    "    output = torch.gather(value, 2, top_k_indices.unsqueeze(-1).expand(-1,-1,-1,value.size(-1)))\n",
    "    return output\n",
    "    \n",
    "class Last_Attention(nn.Module):\n",
    "    def __init__(self, dim, model):\n",
    "        super(Last_Attention, self).__init__()\n",
    "        self.p_dim = 2\n",
    "        self.emb_size = dim\n",
    "        self.head = 8\n",
    "        self.temperature = 0.07\n",
    "        self.head_dim = self.emb_size //self.head\n",
    "        self.Q = nn.Linear(dim,dim)\n",
    "        self.K = nn.Linear(dim,dim)\n",
    "        self.V = nn.Linear(dim,dim)\n",
    "        self.projection = nn.Linear(dim, dim)\n",
    "        self.soft_max = nn.Softmax(dim=-1)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.p_dim*dim, dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 128, bias=False),\n",
    "        )\n",
    "        self.cp = True\n",
    "        self.momentum = 0.1\n",
    "        self.momentum_2 = 0.1\n",
    "        if model == 'bert':\n",
    "            self.register_buffer('running_mean_q', torch.zeros(1,8,160,96))\n",
    "            self.register_buffer('running_std_q', torch.ones(1,8,160,96))\n",
    "            self.register_buffer('running_mean_k', torch.zeros(1,8,160,96))\n",
    "            self.register_buffer('running_std_k', torch.ones(1,8,160,96))\n",
    "            \n",
    "            self.register_buffer('twice_running_mean_q', torch.zeros(1,8,160,96))\n",
    "            self.register_buffer('twice_running_mean_k', torch.zeros(1,8,160,96))\n",
    "        \n",
    "        if model == 'bert_large':\n",
    "            self.register_buffer('running_mean_q', torch.zeros(1,8,160,128))\n",
    "            self.register_buffer('running_std_q', torch.ones(1,8,160,128))\n",
    "            self.register_buffer('running_mean_k', torch.zeros(1,8,160,128))\n",
    "            self.register_buffer('running_std_k', torch.ones(1,8,160,128))\n",
    "            \n",
    "            self.register_buffer('twice_running_mean_q', torch.zeros(1,8,160,128))\n",
    "            self.register_buffer('twice_running_mean_k', torch.zeros(1,8,160,128))\n",
    "    #1, 8, 160, 96\n",
    "    def register_buffer(self, name, tensor):\n",
    "        setattr(self, name, tensor)\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        B, N, C = x.shape\n",
    "        origin_k = self.K(x)\n",
    "        origin_q = self.Q(x)       \n",
    "        origin_v = self.V(x)\n",
    "        \n",
    "        self.running_mean_q = self.running_mean_q.detach()\n",
    "        self.running_std_q = self.running_std_q.detach()\n",
    "        self.running_mean_k = self.running_mean_k.detach()\n",
    "        self.running_std_k = self.running_std_k.detach()\n",
    "        self.twice_running_mean_q = self.twice_running_mean_q.detach()\n",
    "        self.twice_running_mean_k = self.twice_running_mean_k.detach()\n",
    "\n",
    "        \n",
    "        q = origin_q.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        k = origin_k.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        \n",
    "        if training:\n",
    "            q_mean, q_std = torch.mean(q, 0, keepdim=True), torch.std(q, 0, keepdim=True)\n",
    "            k_mean, k_std = torch.mean(k, 0, keepdim=True), torch.std(k, 0, keepdim=True) \n",
    "\n",
    "            self.running_mean_q = (1 - self.momentum) * self.running_mean_q.to(device) + self.momentum * q_mean\n",
    "            self.running_std_q = (1 - self.momentum) * self.running_std_q.to(device) + self.momentum * q_std\n",
    "            self.running_mean_k = (1 - self.momentum) * self.running_mean_k.to(device) + self.momentum * k_mean\n",
    "            self.running_std_k = (1 - self.momentum) * self.running_std_k.to(device) + self.momentum * k_std\n",
    "        else:\n",
    "            q_mean = self.running_mean_q\n",
    "            q_std = self.running_std_q\n",
    "            k_mean = self.running_mean_k\n",
    "            k_std = self.running_std_k\n",
    "        \n",
    "        q = (q - q_mean) /q_std\n",
    "        k = (k - k_mean) /k_std\n",
    "        q = torch.abs(q)\n",
    "        k = torch.abs(k)\n",
    "        \n",
    "        if training:\n",
    "            with torch.no_grad():\n",
    "                q_mean_twice = torch.mean(q, 0, keepdim=True)\n",
    "                k_mean_twice = torch.mean(k, 0, keepdim=True)\n",
    "                self.twice_running_mean_q = (1 - self.momentum_2) * self.twice_running_mean_q.to(device) + self.momentum_2 * q_mean_twice     \n",
    "                self.twice_running_mean_k = (1 - self.momentum_2) * self.twice_running_mean_k.to(device) + self.momentum_2 * k_mean_twice\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_mean_twice = self.twice_running_mean_q\n",
    "                k_mean_twice = self.twice_running_mean_k\n",
    "            q = (q - q_mean_twice)\n",
    "            k = (k - k_mean_twice)\n",
    "            q = torch.abs(q)\n",
    "            k = torch.abs(k)\n",
    "        \n",
    "        v = origin_v.reshape(B,N,self.head, C//self.head).permute(0,2,1,3)\n",
    "        attention = (q @ k.transpose(-2,-1))* (self.head_dim ** (-0.5))\n",
    "        atten = self.soft_max(attention/self.temperature)\n",
    "        out = (atten @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.projection(out)\n",
    "        attentions = atten[:,:, 0, :]\n",
    "\n",
    "        #fairness process\n",
    "        mst_val = choose_value_patch(attentions, v, self.p_dim)\n",
    "        mst_val = mst_val.reshape(B, -1)\n",
    "        mst_val = self.projector(mst_val)\n",
    "        return out, mst_val\n",
    "\n",
    "    \n",
    "class Last_ATBlock(nn.Module):\n",
    "    def __init__(self, dim, model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = Last_Attention(dim, model)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)          \n",
    "        )\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        identity = x\n",
    "        #x = self.norm(x)\n",
    "        x, vz = self.attention(x, training)\n",
    "        x += identity\n",
    "        x = self.norm(x)\n",
    "        res = x \n",
    "        x = self.feedforward(x)\n",
    "        x += res\n",
    "        x = self.norm2(x)\n",
    "        return x, vz\n",
    "    \n",
    "    \n",
    "class BERT_model(nn.Module):\n",
    "    def __init__(self, BERT, dim, model):\n",
    "        super(BERT_model, self).__init__()\n",
    "        self.BERT = BERT\n",
    "        self.last_layer = Last_ATBlock(dim, model)\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 1),     \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, word, attention, training=True):\n",
    "        x, _= self.BERT(word, attention, return_dict= False)\n",
    "        hidden, v = self.last_layer(x, training)\n",
    "        y = self.seq(hidden[:,0])\n",
    "        return y, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091b16da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0.000000 : 100%|████████████████████████████████████████████| 615/615 [04:15<00:00,  2.40batch/s, f=2.98, u=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8986220472440944\n",
      "male TPR 0.8333333333333334\n",
      "DP 0.09515004651269621\n",
      "EOP 0.06528871391076108\n",
      "EoD 0.05244912511757313\n",
      "acc 0.777027027027027\n",
      "Trade off 0.7362726392667506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1.000000 : 100%|████████████████████████████████████████████| 615/615 [04:14<00:00,  2.42batch/s, f=3.25, u=0.634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8877952755905512\n",
      "male TPR 0.7698412698412699\n",
      "DP 0.06651506973076227\n",
      "EOP 0.11795400574928128\n",
      "EoD 0.06992750659781188\n",
      "acc 0.7988565488565489\n",
      "Trade off 0.7429945022656773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2.000000 : 100%|█████████████████████████████████████████████████| 615/615 [04:14<00:00,  2.41batch/s, f=3, u=0.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.843503937007874\n",
      "male TPR 0.7380952380952381\n",
      "DP 0.08601212009846138\n",
      "EOP 0.10540869891263582\n",
      "EoD 0.08862200166831966\n",
      "acc 0.7931392931392931\n",
      "Trade off 0.7228497013794928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3.000000 : 100%|█████████████████████████████████████████████| 615/615 [04:14<00:00,  2.42batch/s, f=2.87, u=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8198818897637795\n",
      "male TPR 0.7063492063492064\n",
      "DP 0.06259115914232893\n",
      "EOP 0.11353268341457312\n",
      "EoD 0.079793686679441\n",
      "acc 0.7900207900207901\n",
      "Trade off 0.7269821186316268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4.000000 : 100%|███████████████████████████████████████████| 615/615 [04:16<00:00,  2.39batch/s, f=2.75, u=0.0889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.875\n",
      "male TPR 0.8412698412698413\n",
      "DP 0.12942779813107008\n",
      "EOP 0.03373015873015872\n",
      "EoD 0.021245280854347848\n",
      "acc 0.7915800415800416\n",
      "Trade off 0.7747627012779773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5.000000 : 100%|████████████████████████████████████████████| 615/615 [04:15<00:00,  2.41batch/s, f=2.72, u=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female TPR 0.8651574803149606\n",
      "male TPR 0.8333333333333334\n",
      "DP 0.12651453378199895\n",
      "EOP 0.03182414698162728\n",
      "EoD 0.029428123800556455\n",
      "acc 0.7910602910602911\n",
      "Trade off 0.7677808708812646\n",
      "***********************************************\n",
      "energy results\n",
      "\n",
      "GPU Utilizations recorded every minute: [0, 93, 99, 83, 99, 99, 81, 97, 99, 98, 99, 99, 99, 84, 97, 99, 99, 98, 100, 97, 94, 98, 99, 98, 99, 98, 100]\n",
      "Average hourly GPU Utilizations: [41.75]\n",
      "\n",
      "Total training time: 1592.91 seconds.\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Changes the seed for reproducibility. \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class SupLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07, base_temperature=None, K=128):\n",
    "        super(SupLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.base_temperature = temperature if base_temperature is None else base_temperature\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        ss = features.shape[0]\n",
    "        batch_size = (features.shape[0] - self.K) \n",
    "\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels[:batch_size], labels.T).float().to(device)\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features[:batch_size], features.T),\n",
    "            self.temperature)\n",
    "\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class SupervisedContrastiveLoss_v2(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Implementation of the loss described in the paper Supervised Contrastive Learning :\n",
    "        https://arxiv.org/abs/2004.11362\n",
    "        :param temperature: int\n",
    "        \"\"\"\n",
    "        super(SupervisedContrastiveLoss_v2, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, projections, targets):\n",
    "        dot_product_tempered = torch.mm(projections, projections.T) / self.temperature\n",
    "        exp_dot_tempered = (\n",
    "            torch.exp(dot_product_tempered - torch.max(dot_product_tempered, dim=1, keepdim=True)[0]) + 1e-5\n",
    "        )\n",
    "\n",
    "        mask_similar_class = (targets.unsqueeze(1).repeat(1, targets.shape[0]) == targets).to(device)\n",
    "        mask_anchor_out = (1 - torch.eye(exp_dot_tempered.shape[0])).to(device)\n",
    "        mask_combined = mask_similar_class * mask_anchor_out\n",
    "        cardinality_per_samples = torch.sum(mask_combined, dim=1)\n",
    "\n",
    "        log_prob = -torch.log(exp_dot_tempered / (torch.sum(exp_dot_tempered * mask_anchor_out, dim=1, keepdim=True)))\n",
    "        supervised_contrastive_loss_per_sample = torch.sum(log_prob * mask_combined, dim=1) / cardinality_per_samples\n",
    "        supervised_contrastive_loss = torch.mean(supervised_contrastive_loss_per_sample)\n",
    "\n",
    "        return supervised_contrastive_loss  \n",
    "    \n",
    "\n",
    "def train_model(moco, BERT, freeze, lr, epoch):\n",
    "    model = BERT_model(BERT, dimension, m).to(device)\n",
    "    if moco:\n",
    "        model_moco = MoCo(model).to(device)\n",
    "        fair_criterion = SupLoss()\n",
    "    else:\n",
    "        fair_criterion = SupervisedContrastiveLoss_v2()\n",
    "        \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    if freeze:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'BERT' in name:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    if moco:\n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model_moco.parameters()), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        \n",
    "    for epoches in range(epoch):\n",
    "        with tqdm(training_data_loader, unit=\"batch\") as tepoch:\n",
    "            model.train()\n",
    "            for word, attention, train_target in tepoch:\n",
    "                # Transfer data to GPU if possible. \n",
    "                word = word.to(device) \n",
    "                attention = attention.to(device)\n",
    "                word = word.squeeze(1)\n",
    "                attention = attention.squeeze(1)\n",
    "                train_target = train_target.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                if moco:\n",
    "                    features, targets, outputs = model_moco(word, attention, train_target, training=True)\n",
    "                    fair_loss = fair_criterion(features.T, targets)\n",
    "                    train_target = train_target.unsqueeze(1)\n",
    "                    ut_loss = criterion(outputs, train_target)\n",
    "                    loss =  ut_loss + 0.1* fair_loss\n",
    "                    tepoch.set_postfix(ul = ut_loss.item(),fl = fair_loss.item())\n",
    "                else:\n",
    "                    outputs, v = model(word, attention)\n",
    "                    train_target = train_target.unsqueeze(1)\n",
    "                    ut_loss = criterion(outputs, train_target)\n",
    "                    fair_loss = fair_criterion(v, train_target.squeeze())\n",
    "                    loss =  ut_loss + 0.8 * fair_loss\n",
    "                    tepoch.set_postfix(u= ut_loss.item(), f= fair_loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tepoch.set_description(f\"epoch %2f \" % epoches)       \n",
    "                \n",
    "        if moco:\n",
    "            model_moco.eval()\n",
    "        else:\n",
    "            model.eval()\n",
    "        test_pred = []\n",
    "        test_gt = []\n",
    "        sense_gt = []\n",
    "        female_predic = []\n",
    "        female_gt = []\n",
    "        male_predic = []\n",
    "        male_gt = []\n",
    "\n",
    "\n",
    "    # Evaluate\n",
    "        for step, (test_word, test_attention, test_target, test_sensitive) in enumerate(test_data_loader):\n",
    "            test_word = test_word.to(device)\n",
    "            test_attention = test_attention.to(device)\n",
    "            test_word = test_word.squeeze(1)\n",
    "            test_attention = test_attention.squeeze(1)\n",
    "            gt = test_target.detach().cpu().numpy()\n",
    "            sen = test_sensitive.detach().cpu().numpy()\n",
    "            test_gt.extend(gt)\n",
    "            sense_gt.extend(sen)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if moco:\n",
    "                    test_pred_ = model_moco._inference(test_word,test_attention)\n",
    "                else:\n",
    "                    test_pred_, _ = model(test_word,test_attention, training=False)\n",
    "                test_pred.extend(torch.round(test_pred_.squeeze(1)).detach().cpu().numpy())\n",
    "\n",
    "        for i in range(len(sense_gt)):\n",
    "            if sense_gt[i] == 0:\n",
    "                female_predic.append(test_pred[i])\n",
    "                female_gt.append(test_gt[i])\n",
    "            else:\n",
    "                male_predic.append(test_pred[i])\n",
    "                male_gt.append(test_gt[i])\n",
    "        female_CM = confusion_matrix(female_gt, female_predic)    \n",
    "        male_CM = confusion_matrix(male_gt, male_predic) \n",
    "        female_dp = (female_CM[1][1]+female_CM[0][1])/(female_CM[0][0]+female_CM[0][1]+female_CM[1][0]+female_CM[1][1])\n",
    "        male_dp = (male_CM[1][1]+male_CM[0][1])/(male_CM[0][0]+male_CM[0][1]+male_CM[1][0]+male_CM[1][1])\n",
    "        female_TPR = female_CM[1][1]/(female_CM[1][1]+female_CM[1][0])\n",
    "        male_TPR = male_CM[1][1]/(male_CM[1][1]+male_CM[1][0])\n",
    "        female_FPR = female_CM[0][1]/(female_CM[0][1]+female_CM[0][0])\n",
    "        male_FPR = male_CM[0][1]/(male_CM[0][1]+male_CM[0][0])\n",
    "\n",
    "        print('Female TPR', female_TPR)\n",
    "        print('male TPR', male_TPR)\n",
    "        print('DP',abs(female_dp - male_dp))\n",
    "        print('EOP', abs(female_TPR - male_TPR))\n",
    "        print('EoD',0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR)))\n",
    "        print('acc', accuracy_score(test_gt, test_pred))\n",
    "        print('Trade off',accuracy_score(test_gt, test_pred)*(1-0.5*(abs(female_FPR-male_FPR)+ abs(female_TPR-male_TPR))) )\n",
    "\n",
    "\n",
    "        \n",
    "###add those code before your training loop\n",
    "import subprocess  #past to your code\n",
    "import threading  #past to your code\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def compute_average_hourly_utilization(minute_utilizations):\n",
    "    # Calculate the number of zeros needed to pad the list to a multiple of 60\n",
    "    padding_length = (60 - len(minute_utilizations) % 60) % 60  # This ensures no padding if length is already a multiple of 60\n",
    "    \n",
    "    # Pad the utilization list with zeros\n",
    "    padded_utilizations = minute_utilizations + [0] * padding_length\n",
    "    \n",
    "    num_hours = len(padded_utilizations) // 60\n",
    "    \n",
    "    # Compute the hourly averages\n",
    "    hourly_averages = [sum(padded_utilizations[i*60:(i+1)*60])/60.0 for i in range(num_hours)]\n",
    "\n",
    "    return hourly_averages\n",
    "\n",
    "def get_gpu_utilization(device_id):\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=utilization.gpu\", \"--format=csv,noheader,nounits\"]\n",
    "        ).decode(\"utf-8\").strip().split('\\n')\n",
    "        return int(result[device_id])############!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying GPU utilization: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to repeatedly track GPU utilization every minute\n",
    "def track_gpu_utilization_periodically(device_id):\n",
    "    if not stop_tracking:\n",
    "        utilization = get_gpu_utilization(device_id)\n",
    "        if utilization is not None:\n",
    "            utilization_list.append(utilization)\n",
    "        threading.Timer(60, track_gpu_utilization_periodically, args=[device_id]).start()\n",
    "\n",
    "# ... [rest of the PyTorch code for model, optimizer, etc.]\n",
    "\n",
    "utilization_list = []\n",
    "stop_tracking = False\n",
    "\n",
    "# Start the periodic GPU tracking\n",
    "track_gpu_utilization_periodically(device_id)\n",
    "start_time = time.time()\n",
    "        \n",
    "seed_everything(4096)\n",
    "train_model(moco, bert, freeze, lr, epoch)\n",
    "\n",
    "\n",
    "stop_tracking = True\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Print GPU utilizations\n",
    "print(\"***********************************************\")\n",
    "print(\"energy results\")\n",
    "print(\"\\nGPU Utilizations recorded every minute:\", utilization_list)\n",
    "\n",
    "with open('utilization_data.pkl', 'wb') as f:\n",
    "    pickle.dump(utilization_list, f)\n",
    "    \n",
    "average_hourly_utilizations = compute_average_hourly_utilization(utilization_list)\n",
    "print(\"Average hourly GPU Utilizations:\", average_hourly_utilizations)\n",
    "print(f\"\\nTotal training time: {total_time:.2f} seconds.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLcourse]",
   "language": "python",
   "name": "conda-env-DLcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
